{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPA8x6QSPxT/I8a8puTJkvt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlinapp/proofLLM/blob/main/LLMArch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mmC3ECChK8Wg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,      # vocab size\n",
        "    \"context_length\": 1024,   # context lnegth\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,            # Number of attention heads\n",
        "    \"n_layers\": 12,           # Number of layers\n",
        "    \"drop_rate\": 0.1,         # Dropout rate\n",
        "    \"qkv_bias\": False         # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "kdu4bke4LFG3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyGPTModel(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
        "    self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
        "    self.final_norm = DummyLayerNorm(config[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_len = x.shape\n",
        "    tok_emb = self.tok_emb(x)\n",
        "    pos_emb = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "  def forward(self, x):\n",
        "    return x\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "  def forward(self, x):\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "srP9MuKOL8BD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "E0Z_T9UNHs_-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim = 0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE22tEsAHv1E",
        "outputId": "456c8997-97ce-4291-a2f5-1f4993dc3b23"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "logits = model(batch)\n",
        "print(\"Ouptut shape:\", logits.shape)\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRBpmc4ZH3Vh",
        "outputId": "de752efd-d9c8-4808-d829-8cdca8259722"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ouptut shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
            "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
            "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
            "         [ 0.0448,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
            "\n",
            "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
            "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
            "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
            "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "batch_example = torch.randn(2, 5)\n",
        "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
        "out = layer(batch_example)\n",
        "print(batch_example)\n",
        "print(out)\n",
        "mean = out.mean(dim=-1, keepdim=True)\n",
        "var = out.var(dim=-1, keepdim=True)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Var:\\n\", var)\n",
        "out_norm = (out - mean) / torch.sqrt(var + 1e-5)\n",
        "mean, var = out_norm.mean(dim=-1, keepdim=True), out_norm.var(dim=-1, keepdim=True)\n",
        "print(\"Normalized:\\n\", out_norm)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Var:\\n\", var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAhRb7kpxoJr",
        "outputId": "f6a30cb7-766f-4e46-e134-9a3e4f1d1e80"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
            "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n",
            "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
            "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "Mean:\n",
            " tensor([[0.1324],\n",
            "        [0.2170]], grad_fn=<MeanBackward1>)\n",
            "Var:\n",
            " tensor([[0.0231],\n",
            "        [0.0398]], grad_fn=<VarBackward0>)\n",
            "Normalized:\n",
            " tensor([[ 0.6157,  1.4123, -0.8717,  0.5871, -0.8717, -0.8717],\n",
            "        [-0.0189,  0.1121, -1.0875,  1.5171,  0.5647, -1.0875]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "Mean:\n",
            " tensor([[-1.9868e-08],\n",
            "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
            "Var:\n",
            " tensor([[0.9996],\n",
            "        [0.9997]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ln = LayerNorm(emb_dim=5)\n",
        "out_ln = ln(batch_example)\n",
        "mean = out_ln.mean(dim=-1, keepdim=True)\n",
        "var = out_ln.var(dim=-1, keepdim=True, unbiased=False)\n",
        "print(\"Normalized:\\n\", out_ln)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Var:\\n\", var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYxb3_EN2jxh",
        "outputId": "8ea93270-35e5-448c-eee8-eed55f7c8a52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized:\n",
            " tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
            "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n",
            "Mean:\n",
            " tensor([[-2.9802e-08],\n",
            "        [ 0.0000e+00]], grad_fn=<MeanBackward1>)\n",
            "Var:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(nn.Linear(config[\"emb_dim\"], 4 * config[\"emb_dim\"]),\n",
        "                                GELU(),\n",
        "                                nn.Linear(4 * config[\"emb_dim\"], config[\"emb_dim\"]))\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "class ExampleDeepNeuralNetwork(nn.Module):\n",
        "  def __init__(self, layer_sizes, use_shortcut):\n",
        "    super().__init__()\n",
        "    self.use_shortcut = use_shortcut\n",
        "    self.layers = nn.ModuleList()\n",
        "    # for i in range(len(layer_sizes)-1):\n",
        "    #   self.layers.append(nn.Sequential(nn.Linear(layer_sizes[i], layer_sizes[i + 1]), GELU()))\n",
        "    #   #print(i)\n",
        "    self.layers = nn.ModuleList([\n",
        "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
        "        ])\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      layer_output = layer(x)\n",
        "      if self.use_shortcut and x.shape == layer_output.shape:\n",
        "        x = x + layer_output\n",
        "      else:\n",
        "        x = layer_output\n",
        "    return x\n",
        "\n",
        "\n",
        "def print_gradients(model, x):\n",
        "  output = model(x)\n",
        "  target = torch.tensor([[0.]])\n",
        "  loss = nn.MSELoss()\n",
        "  loss = loss(output, target)\n",
        "  loss.backward()\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      print(f\"{name} has gradient mean of: {param.grad.abs().mean().item()}\")"
      ],
      "metadata": {
        "id": "gtlUD06d4wLc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffn = FeedForward(GPT_CONFIG_124M)\n",
        "x = torch.randn(2, 3, GPT_CONFIG_124M[\"emb_dim\"])\n",
        "print(x.shape)\n",
        "out_ffn = ffn(x)\n",
        "print(out_ffn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOO-ohRQ4zGF",
        "outputId": "b898bf7c-28d2-4d19-ae56-999f9deae4da"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 768])\n",
            "tensor([[[-0.3731, -0.2161,  0.1972,  ..., -0.2462,  0.0535,  0.2413],\n",
            "         [ 0.0069,  0.0609,  0.3952,  ...,  0.1626, -0.0415, -0.1237],\n",
            "         [ 0.1569, -0.1565, -0.0789,  ..., -0.3007,  0.2389, -0.1702]],\n",
            "\n",
            "        [[ 0.2887,  0.0783,  0.1038,  ..., -0.2605, -0.0504, -0.2268],\n",
            "         [-0.0889,  0.2274,  0.0563,  ..., -0.2062,  0.0148, -0.2420],\n",
            "         [ 0.2520, -0.0005, -0.2848,  ..., -0.0739, -0.0354,  0.0410]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes = [3,3,3,3,3,1]\n",
        "sample_input = torch.tensor([[1.,0.,-1.]])\n",
        "torch.manual_seed(123)\n",
        "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
        "print(\"No skip connections. Vanishing gradients:\\n\")\n",
        "print_gradients(model_without_shortcut, sample_input)\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
        "print(\"\\nWith skip connections. Non-vanishing gradients:\\n\")\n",
        "print_gradients(model_with_shortcut, sample_input)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pojZpugP8WHz",
        "outputId": "6d7f12c4-92ea-4fb9-d1b9-a4961edc1ccf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No skip connections. Vanishing gradients:\n",
            "\n",
            "layers.0.0.weight has gradient mean of: 0.00020173584925942123\n",
            "layers.1.0.weight has gradient mean of: 0.00012011159560643137\n",
            "layers.2.0.weight has gradient mean of: 0.0007152040489017963\n",
            "layers.3.0.weight has gradient mean of: 0.0013988736318424344\n",
            "layers.4.0.weight has gradient mean of: 0.005049645435065031\n",
            "\n",
            "With skip connections. Non-vanishing gradients:\n",
            "\n",
            "layers.0.0.weight has gradient mean of: 0.22169792652130127\n",
            "layers.1.0.weight has gradient mean of: 0.20694108307361603\n",
            "layers.2.0.weight has gradient mean of: 0.3289699852466583\n",
            "layers.3.0.weight has gradient mean of: 0.2665732204914093\n",
            "layers.4.0.weight has gradient mean of: 1.3258541822433472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from attention import MultiHeadAttention\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.attn = MultiHeadAttention(d_in = config[\"emb_dim\"], d_out=config[\"emb_dim\"],\n",
        "                                   context_length = config[\"context_length\"],\n",
        "                                   dropout = config[\"drop_rate\"],\n",
        "                                   num_heads = config[\"n_heads\"],\n",
        "                                   qkv_bias =config[\"qkv_bias\"])\n",
        "\n",
        "    # using two separate norms since they have different scale weights and biases\n",
        "    self.ff = FeedForward(config)\n",
        "    self.norm1 = LayerNorm(config[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(config[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # shortcut for attention block\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.attn(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = shortcut + x\n",
        "\n",
        "    # shortcut for feed forward block\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = shortcut + x\n",
        "    return x"
      ],
      "metadata": {
        "id": "f4TVJDccFk9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2dc9c0-b4d9-4a62-a036-2d08a3b81d14"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.6.0+cu124\n",
            "torch.Size([]) torch.Size([3])\n",
            "torch.Size([]) torch.Size([3])\n",
            "torch.Size([]) torch.Size([3])\n",
            "torch.Size([]) torch.Size([3])\n",
            "torch.Size([]) torch.Size([3])\n",
            "torch.Size([]) torch.Size([3])\n",
            "This is atten_weights_2:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
            "x2:  tensor([0.5500, 0.8700, 0.6600])\n",
            "Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]], requires_grad=True) Parameter containing:\n",
            "tensor([[0.0756, 0.1966],\n",
            "        [0.3164, 0.4017],\n",
            "        [0.1186, 0.8274]], requires_grad=True) Parameter containing:\n",
            "tensor([[0.1366, 0.1025],\n",
            "        [0.1841, 0.7264],\n",
            "        [0.3153, 0.6871]], requires_grad=True)\n",
            "qyery2:  tensor([0.4306, 1.4551], grad_fn=<SqueezeBackward4>)\n",
            "atten_score_2:  tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
            "       grad_fn=<SqueezeBackward4>)\n",
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "tensor([0.3061, 0.8210], grad_fn=<SqueezeBackward4>)\n",
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n",
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n",
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n",
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n",
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "mask:  tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n",
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.]]) tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n",
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n",
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
            "torch.Size([2, 6, 4])\n",
            "tensor([[[0.4300, 0.1500, 0.8900],\n",
            "         [0.5500, 0.8700, 0.6600],\n",
            "         [0.5700, 0.8500, 0.6400],\n",
            "         [0.2200, 0.5800, 0.3300],\n",
            "         [0.7700, 0.2500, 0.1000],\n",
            "         [0.0500, 0.8000, 0.5500]],\n",
            "\n",
            "        [[0.4300, 0.1500, 0.8900],\n",
            "         [0.5500, 0.8700, 0.6600],\n",
            "         [0.5700, 0.8500, 0.6400],\n",
            "         [0.2200, 0.5800, 0.3300],\n",
            "         [0.7700, 0.2500, 0.1000],\n",
            "         [0.0500, 0.8000, 0.5500]]])\n",
            "Inside of print name! Joe\n",
            "This is input:  90\n",
            "tensor([[[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]],\n",
            "\n",
            "        [[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape:  torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "x = torch.rand(2, 4, 768)\n",
        "block = TransformerBlock(GPT_CONFIG_124M)\n",
        "out = block(x)\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Output shape:\", out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33aXewh5IaPb",
        "outputId": "611d283c-c556-4fae-edb1-fa7696f68771"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4, 768])\n",
            "Output shape: torch.Size([2, 4, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from attention import MultiHeadAttention\n",
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
        "    self.trf_blocks = nn.Sequential(*[TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n",
        "    self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_len = x.shape\n",
        "    tok_emb = self.tok_emb(x)\n",
        "    pos_emb = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_x + self.shift\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.attn = MultiHeadAttention(d_in = config[\"emb_dim\"], d_out=config[\"emb_dim\"],\n",
        "                                   context_length = config[\"context_length\"],\n",
        "                                   dropout = config[\"drop_rate\"],\n",
        "                                   num_heads = config[\"n_heads\"],\n",
        "                                   qkv_bias =config[\"qkv_bias\"])\n",
        "\n",
        "    # using two separate norms since they have different scale weights and biases\n",
        "    self.ff = FeedForward(config)\n",
        "    self.norm1 = LayerNorm(config[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(config[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # shortcut for attention block\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.attn(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = shortcut + x\n",
        "\n",
        "    # shortcut for feed forward block\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = shortcut + x\n",
        "    return x"
      ],
      "metadata": {
        "id": "nVpYUjs4Jwjv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "print(\"Input Batch:\\n\", batch)\n",
        "logits = model(batch)\n",
        "print(\"\\nOuptut shape:\\n\", logits.shape)\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFatR9hGK1NN",
        "outputId": "fed4206b-486f-4903-ad91-fabc2f2047e1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Ouptut shape:\n",
            " torch.Size([2, 4, 50257])\n",
            "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
            "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
            "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
            "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
            "\n",
            "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
            "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
            "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
            "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")\n",
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)\n",
        "\n",
        "block = TransformerBlock(GPT_CONFIG_124M)\n",
        "ff_params = sum(p.numel() for p in block.ff.parameters())\n",
        "print(f\"Total number of ff_parameters: {ff_params:,}\")\n",
        "aa_params = sum(p.numel() for p in block.attn.parameters())\n",
        "print(f\"Total number of aa_parameters: {aa_params:,}\")\n",
        "\n",
        "total_size_bytes = total_params * 4  # assume floating point 32, so 4 bytes per parameter\n",
        "mb = total_size_bytes / (1024 * 1024)\n",
        "print(f\"Total size of parameters: {mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekG1JHCRNl0Q",
        "outputId": "4ab858e1-9e85-4068-ff16-562f448a5e1d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 163,009,536\n",
            "Token embedding layer shape: torch.Size([50257, 768])\n",
            "Output layer shape: torch.Size([50257, 768])\n",
            "Total number of ff_parameters: 4,722,432\n",
            "Total number of aa_parameters: 2,360,064\n",
            "Total size of parameters: 621.83 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx,  max_new_tokens, context_size):\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "\n",
        "    logits = logits[:, -1, :]\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "  return idx"
      ],
      "metadata": {
        "id": "1VG3pkW_UzFA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_context = \"Hello, I am\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "print(\"encoded:\", encoded)\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "print(\"encoded_tensor:\", encoded_tensor)\n",
        "\n",
        "model.eval()\n",
        "out = generate_text_simple(model, encoded_tensor, max_new_tokens=6, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
        "print(out)\n",
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kehmoH69WNTp",
        "outputId": "2d3eaecf-6af3-4c35-9d98-12505c5fbffe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded: [15496, 11, 314, 716]\n",
            "encoded_tensor: tensor([[15496,    11,   314,   716]])\n",
            "tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
            "Hello, I am Featureiman Byeswickattribute argue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,      # vocab size\n",
        "    \"context_length\": 256,   # context lnegth\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,            # Number of attention heads\n",
        "    \"n_layers\": 12,           # Number of layers\n",
        "    \"drop_rate\": 0.1,         # Dropout rate\n",
        "    \"qkv_bias\": False         # Query-Key-Value bias\n",
        "}\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  encoded_tensor = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "  encoded_tensor = torch.tensor(encoded_tensor).unsqueeze(0)\n",
        "  return encoded_tensor\n",
        "def token_id_to_text(token_ids, tokenizer):\n",
        "  decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
        "  return decoded_text\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "encoded_tensor = text_to_token_ids(start_context, tokenizer)\n",
        "token_ids = generate_text_simple(model, encoded_tensor, max_new_tokens=10, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
        "print(token_id_to_text(token_ids, tokenizer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C19XnbJWZWv7",
        "outputId": "447f93bc-14ba-4203-aca0-ddd5b316e342"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        text_data = response.read().decode('utf-8')\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "else:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()"
      ],
      "metadata": {
        "id": "yVsKJLl0OEpZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_data[:99])\n",
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "print(f\"Total characters: {total_characters}\")\n",
        "print(f\"Total tokens: {total_tokens}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unGKNMsLO54K",
        "outputId": "7e5d8874-da7b-45d5-de9c-aff209ec94b0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
            "Total characters: 20479\n",
            "Total tokens: 5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.9\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "print(\"Train data length:\", len(train_data))\n",
        "val_data = text_data[split_idx:]\n",
        "print(\"Val data length:\", len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpEiXd8rQUoU",
        "outputId": "8e2b1d06-dd04-4e0a-e10f-71e7b736f5b4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data length: 18431\n",
            "Val data length: 2048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenization import createDataLoaderV1\n",
        "torch.manual_seed(123)\n",
        "# BOTH CONSUME random numbers!!!!!! Thus if you set shuffle=True for val_loader they will both consume random numbers from the same RNG, so val_loader\n",
        "# indirectly affects train_loader!!!\n",
        "train_loader = createDataLoaderV1(train_data,\n",
        "                                  batch_size=2,\n",
        "                                  max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "                                  stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "                                  shuffle=True,\n",
        "                                  drop_last=True,\n",
        "                                  num_workers=0)\n",
        "val_loader = createDataLoaderV1(val_data,\n",
        "                                batch_size=2,\n",
        "                                max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "                                stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "                                shuffle=False,\n",
        "                                drop_last=False,\n",
        "                                num_workers=0)\n",
        "print(len(val_loader))\n",
        "print(\"Train_loader\")\n",
        "for x, y in train_loader:\n",
        "  print(x.shape, y.shape)\n",
        "print(\"\\nVal_loader\")\n",
        "for x, y in val_loader:\n",
        "  print(x.shape, y.shape)\n",
        "\n",
        "\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "  logits = model(input_batch)\n",
        "  loss = nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "  return loss\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "  total_loss = 0\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(len(data_loader), num_batches)\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):   # BOTH CONSUME random numbers!!!!!!\n",
        "    if i < num_batches:\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss / num_batches\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77xSyobqQlsW",
        "outputId": "0e521e8a-9fe3-4cef-9bd0-c3c2d472ebad"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Train_loader\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Val_loader\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(device)\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model, device)\n",
        "  val_loss = calc_loss_loader(val_loader, model, device)\n",
        "print(f\"Train loss: {train_loss:.10f}\")\n",
        "print(f\"Val loss: {val_loss:.10f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AoVt-t6p1v2",
        "outputId": "7df2c840-56fd-4a57-e0d8-dd8d2c0d57e6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Train loss: 10.9875834783\n",
            "Val loss: 10.9811048508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, eval_iter, device):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "  model.train()\n",
        "  return train_loss, val_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  encoded_tensor = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "  encoded_tensor = torch.tensor(encoded_tensor).unsqueeze(0)\n",
        "  return encoded_tensor\n",
        "\n",
        "\n",
        "def token_id_to_text(token_ids, tokenizer):\n",
        "  decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
        "  return decoded_text\n",
        "\n",
        "\n",
        "def generate_text_simple(model, idx,  max_new_tokens, context_size):\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "\n",
        "    logits = logits[:, -1, :]\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "  return idx\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, start_context, device):\n",
        "  model.eval()\n",
        "  context_size = model.pos_emb.weight.shape[0]    # num_positions\n",
        "\n",
        "  encoded_tensor = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "    token_ids = generate_text_simple(model, encoded_tensor, max_new_tokens=50, context_size=context_size)\n",
        "\n",
        "  decoded_text = token_id_to_text(token_ids, tokenizer)\n",
        "  print(decoded_text.replace(\"\\n\", \" \"))\n",
        "  model.train()\n",
        "\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq,\n",
        "                       eval_iter, start_context, tokenizer):\n",
        "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "  tokens_seen, global_step = 0, -1\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()   # enables dropout and batchnorm. Only affects layers that already exist in the model, so if no dropout module in model, nothing to enable\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      tokens_seen += input_batch.numel()\n",
        "      global_step += 1\n",
        "\n",
        "\n",
        "      if (global_step % eval_freq == 0):\n",
        "        train_loss, val_loss = evaluate_model(model, train_loader, val_loader, eval_iter, device)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(tokens_seen)\n",
        "        print(f\"Epoch {epoch + 1} (step {global_step:06d}) | Train loss {train_loss:.3f} | Val loss {val_loss:.3f} \")\n",
        "\n",
        "    generate_and_print_sample(model, tokenizer, start_context, device)\n",
        "  return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "v0NyIlk7q_WK"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "#                        eval_freq, eval_iter, start_context, tokenizer):\n",
        "#     # Initialize lists to track losses and tokens seen\n",
        "#     train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "#     tokens_seen, global_step = 0, -1\n",
        "\n",
        "#     # Main training loop\n",
        "#     for epoch in range(num_epochs):\n",
        "#         model.train()  # Set model to training mode\n",
        "\n",
        "#         for input_batch, target_batch in train_loader:\n",
        "#             optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "#             loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "#             loss.backward() # Calculate loss gradients\n",
        "#             optimizer.step() # Update model weights using loss gradients\n",
        "#             tokens_seen += input_batch.numel()\n",
        "#             global_step += 1\n",
        "\n",
        "#             # Optional evaluation step\n",
        "#             if global_step % eval_freq == 0:\n",
        "#                 train_loss, val_loss = evaluate_model(\n",
        "#                     model, train_loader, val_loader, device, eval_iter)\n",
        "#                 train_losses.append(train_loss)\n",
        "#                 val_losses.append(val_loss)\n",
        "#                 track_tokens_seen.append(tokens_seen)\n",
        "#                 print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "#                       f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "#         # Print a sample text after each epoch\n",
        "#         generate_and_print_sample(\n",
        "#             model, tokenizer, device, start_context\n",
        "#         )\n",
        "\n",
        "#     return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "# def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "#         val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "#     model.train()\n",
        "#     return train_loss, val_loss\n",
        "# def token_ids_to_text(token_ids, tokenizer):\n",
        "#   decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
        "#   return decoded_text\n",
        "\n",
        "# def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "#     model.eval()\n",
        "#     context_size = model.pos_emb.weight.shape[0]\n",
        "#     encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "#     with torch.no_grad():\n",
        "#         token_ids = generate_text_simple(\n",
        "#             model=model, idx=encoded,\n",
        "#             max_new_tokens=50, context_size=context_size\n",
        "#         )\n",
        "#     decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "#     print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "#     model.train()\n",
        "torch.manual_seed(42)  # IMPORTANT — reset all randomness here\n",
        "train_loader1 = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader1 = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "val_loader2 = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# def evaluate_model(model, train_loader, val_loader, eval_iter, device):\n",
        "#   model.eval()\n",
        "#   with torch.no_grad():\n",
        "#     train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "#     val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "#   model.train()\n",
        "#   return train_loss, val_loss\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    train_loss_1, val_loss_1 = evaluate_model(model, train_loader1, val_loader1, 5, device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    train_loss_2, val_loss_2 = evaluate_model(model, train_loader1, val_loader2, 5, device)\n",
        "\n",
        "print(\"Train loss with unshuffled val_loader:\", train_loss_1, val_loss_1)\n",
        "print(\"Train loss with shuffled val_loader:\", train_loss_2, val_loss_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuDFnuX2AxEI",
        "outputId": "7a92154d-0205-4ad3-ee75-ae606050ca80"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss with unshuffled val_loader: 0.5846027433872223 6.443647384643555\n",
            "Train loss with shuffled val_loader: 0.590851879119873 6.443647384643555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "num_epochs = 10\n",
        "start_context = \"Every effort moves you\"\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(model, train_loader, val_loader,\n",
        "                                                           optimizer, device=device, num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "                                                           start_context=start_context, tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "l2P8oLilxzcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ebc916-29f4-4e4d-a558-fabaec1f593a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (step 000000) | Train loss 9.818 | Val loss 9.930 \n",
            "Epoch 1 (step 000005) | Train loss 8.066 | Val loss 8.336 \n",
            "Every effort moves you,,,,,,,,,,,,.                                     \n",
            "Epoch 2 (step 000010) | Train loss 6.623 | Val loss 7.053 \n",
            "Epoch 2 (step 000015) | Train loss 6.047 | Val loss 6.605 \n",
            "Every effort moves you, and,, and,,,,,,, and,.                                   \n",
            "Epoch 3 (step 000020) | Train loss 5.532 | Val loss 6.507 \n",
            "Epoch 3 (step 000025) | Train loss 5.399 | Val loss 6.389 \n",
            "Every effort moves you, and to the to the of the to the, and I had. Gis, and, and, and, and, and, and I had the, and, and, and, and, and, and, and, and, and\n",
            "Epoch 4 (step 000030) | Train loss 4.895 | Val loss 6.280 \n",
            "Epoch 4 (step 000035) | Train loss 4.648 | Val loss 6.304 \n",
            "Every effort moves you.  \"I the picture.                    \"I\"I the picture\"I had the the honour of the picture and I had been the picture of\n",
            "Epoch 5 (step 000040) | Train loss 4.023 | Val loss 6.165 \n",
            "Every effort moves you know                                                 \n",
            "Epoch 6 (step 000045) | Train loss 3.625 | Val loss 6.172 \n",
            "Epoch 6 (step 000050) | Train loss 3.045 | Val loss 6.144 \n",
            "Every effort moves you know the was his a little the.  \"I had the last word.           \"Oh, and I had a little.   \"I looked, and I had a little of\n",
            "Epoch 7 (step 000055) | Train loss 2.948 | Val loss 6.183 \n",
            "Epoch 7 (step 000060) | Train loss 2.230 | Val loss 6.128 \n",
            "Every effort moves you know the picture to have been too--I felt, and Mrs.  \"I was no--and the fact, and that, and I was his pictures.  \"I looked up his pictures--and--because he was a little\n",
            "Epoch 8 (step 000065) | Train loss 1.774 | Val loss 6.162 \n",
            "Epoch 8 (step 000070) | Train loss 1.475 | Val loss 6.229 \n",
            "Every effort moves you?\"  \"Yes--I glanced after him, and uncertain.  \"I looked up, and the fact, and to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n",
            "Epoch 9 (step 000075) | Train loss 1.135 | Val loss 6.268 \n",
            "Epoch 9 (step 000080) | Train loss 0.858 | Val loss 6.298 \n",
            "Every effort moves you?\"  \"Yes--quite insensible to the fact with the last word.    \"I looked, and that, and I remember getting off a prodigious phrase about the honour being _mine_--because he's the first\n",
            "Epoch 10 (step 000085) | Train loss 0.627 | Val loss 6.382 \n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "token_ids = generate_text_simple(model, text_to_token_ids(\"Every effort moves you\", tokenizer).to(device), max_new_tokens=25, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
        "print(token_id_to_text(token_ids, tokenizer))\n",
        "\n",
        "vocab = {\n",
        "    \"closer\" : 0,\n",
        "    \"every\" : 1,\n",
        "    \"effort\" : 2,\n",
        "    \"forward\" : 3,\n",
        "    \"inches\" : 4,\n",
        "    \"moves\" : 5,\n",
        "    \"pizza\" : 6,\n",
        "    \"toward\": 7,\n",
        "    \"you\" : 8\n",
        "}\n",
        "\n",
        "inverse_vocab = {v : k for k, v in vocab.items()}\n",
        "print(vocab)\n",
        "print(\"\\nInverse Vocab:\\n\", inverse_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xQPdmNK-qAo",
        "outputId": "4d07df6a-8b74-46a4-ff1f-0361f552a2ab"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every effort moves you?\"\n",
            "\n",
            "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
            "\n",
            "\n",
            "{'closer': 0, 'every': 1, 'effort': 2, 'forward': 3, 'inches': 4, 'moves': 5, 'pizza': 6, 'toward': 7, 'you': 8}\n",
            "\n",
            "Inverse Vocab:\n",
            " {0: 'closer', 1: 'every', 2: 'effort', 3: 'forward', 4: 'inches', 5: 'moves', 6: 'pizza', 7: 'toward', 8: 'you'}\n"
          ]
        }
      ]
    }
  ]
}