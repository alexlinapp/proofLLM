{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMj1HGn2YW8f2ARaOBwCLoa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlinapp/proofLLM/blob/main/finetuning_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8PNBof0eJDU",
        "outputId": "8d25181b-d554-4adc-9c9a-6b6b525bdec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, nothing should download from this.\n",
            "File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "print(\"Hello, nothing should download from this.\")\n",
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "  if data_file_path.exists():\n",
        "    print(f\"Data file already exists at {data_file_path}. Skipping download and extraction.\")\n",
        "    return\n",
        "\n",
        "  # downloads the file\n",
        "  with urllib.request.urlopen(url) as response:\n",
        "    with open(zip_path, \"wb\") as zip_file:\n",
        "      zip_file.write(response.read())\n",
        "\n",
        "  # unzips the file\n",
        "  with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "\n",
        "  original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "  os.rename(original_file_path, data_file_path)\n",
        "  print(f\"File downloaded and saved as {data_file_path}\")\n",
        "\n",
        "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgk9rI5vq6Rt",
        "outputId": "b28fbd66-7902-4aec-b35a-ae1324ad9f00"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n",
        "df\n",
        "print(df[\"label\"].value_counts())\n",
        "\n",
        "\n",
        "def create_balanced_dataset(df):\n",
        "  num_spam = df[df[\"label\"] == \"spam\"].shape[0]\n",
        "  ham_subset = df[df[\"label\"] == \"ham\"].sample(n=num_spam, random_state=123)\n",
        "  balanced_df = pd.concat([ham_subset, df[df[\"label\"] == \"spam\"]])\n",
        "  return balanced_df\n",
        "\n",
        "def random_split(df, train_frac, validation_frac):\n",
        "  df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "  train_end = int(train_frac * len(df))\n",
        "  validation_end = train_end + int(validation_frac * len(df))\n",
        "\n",
        "\n",
        "  train_df = df[:train_end]\n",
        "  validation_df = df[train_end:validation_end]\n",
        "  test_df = df[validation_end:]\n",
        "\n",
        "  return train_df, validation_df, test_df\n",
        "\n",
        "\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "print(balanced_df[\"label\"].value_counts())\n",
        "\n",
        "balanced_df[\"label\"] = balanced_df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "\n",
        "train_df.to_csv(\"train.tsv\", index=None)\n",
        "validation_df.to_csv(\"validation.tsv\", index=None)\n",
        "test_df.to_csv(\"test.tsv\", index=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ypg2cYJi7Xo",
        "outputId": "20678cce-7b21-4f8e-980a-990aa9c56907"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n",
            "label\n",
            "ham     747\n",
            "spam    747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "  def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
        "    self.data = pd.read_csv(csv_file)\n",
        "    self.encoded_texts = [tokenizer.encode(text) for text in self.data['text']]\n",
        "    if max_length is None:\n",
        "      self.max_length = self.__longest_encoded_length()\n",
        "    else:\n",
        "      self.max_length = max_length\n",
        "      self.encoded_texts = [encoded_text[:self.max_length] for encoded_text in self.encoded_texts]\n",
        "\n",
        "    self.encoded_texts = [encoded_text + [pad_token_id] * (self.max_length - len(encoded_text)) for encoded_text in self.encoded_texts]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    encoded = self.encoded_texts[idx]\n",
        "    label = self.data.iloc[idx][\"label\"]\n",
        "    return (torch.tensor(encoded), torch.tensor(label))\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __longest_encoded_length(self):\n",
        "    max_length = 0\n",
        "    for encoded_text in self.encoded_texts:\n",
        "      max_length = max(max_length, len(encoded_text))\n",
        "    return max_length"
      ],
      "metadata": {
        "id": "QoZsJNEukt67"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset= SpamDataset(csv_file=\"train.tsv\", tokenizer=tokenizer)\n",
        "print(train_dataset.max_length)\n",
        "validation_dataset = SpamDataset(\"validation.tsv\", tokenizer, max_length=train_dataset.max_length)\n",
        "print(validation_dataset.max_length)\n",
        "test_dataset = SpamDataset(\"test.tsv\", tokenizer, max_length=train_dataset.max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbQu_lzoqubQ",
        "outputId": "dfb07b9c-09ad-4b8f-cabf-dbcb9400dff6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n",
            "120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8;\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "validation_loader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
        "\n",
        "x = next(iter(train_loader))\n",
        "next(iter(train_loader))[0].shape, next(iter(train_loader))[1].shape\n",
        "\n",
        "\n",
        "print(f\"len(train_loader): {len(train_loader)}\")\n",
        "print(f\"len(validation_loader): {len(validation_loader)}\")\n",
        "print(f\"len(test_loader): {len(test_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC84ghdztZR1",
        "outputId": "c2b5d015-698d-4cad-fa80-3353b53ec249"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_loader): 130\n",
            "len(validation_loader): 19\n",
            "len(test_loader): 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "        \"vocab_size\" : 50257,\n",
        "        \"context_length\" : 1024,\n",
        "        \"drop_rate\" : 0.0,\n",
        "        \"qkv_bias\"  : True\n",
        "}\n",
        "model_configs = {\n",
        " \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        " \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        " \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        " \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
      ],
      "metadata": {
        "id": "lDlI0rd_xTCK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "Download boilerplate code from gpt_download.py\n",
        "\n",
        "'''\n",
        "\n",
        "import urllib.request\n",
        "url = (\n",
        " \"https://raw.githubusercontent.com/rasbt/\"\n",
        " \"LLMs-from-scratch/main/ch05/\"\n",
        " \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split('/')[-1]\n",
        "file_name, _ = urllib.request.urlretrieve(url, filename)\n",
        "from gpt_download import download_and_load_gpt2\n",
        "from previous_chapters import GPTModel, load_weights_into_gpt, generate_text_simple, text_to_token_ids, token_ids_to_text\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(' ')[-1].lstrip('(').rstrip(')')\n",
        "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();   # use semicolon to suppress the output display"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-7Zzq4YxmOw",
        "outputId": "cfab6f0d-ea18-4b55-9d64-d1f777e473a3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = \"Every effort moves you\"\n",
        "token_ids = generate_text_simple(model=model, idx=text_to_token_ids(text_1, tokenizer),\n",
        "                                max_new_tokens=15,\n",
        "                                 context_size=BASE_CONFIG[\"context_length\"])\n",
        "print(token_ids_to_text(token_ids, tokenizer))\n",
        "\n",
        "text_2 = (\n",
        "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
        "    \" 'You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
        ")\n",
        "\n",
        "token_ids = generate_text_simple(model=model, idx=text_to_token_ids(text_2, tokenizer),\n",
        "                                 max_new_tokens=15,\n",
        "                                 context_size=BASE_CONFIG[\"context_length\"])\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEJ2U8BRLYDs",
        "outputId": "4310719a-4fea-49ad-fe3c-19f13e048ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every effort moves you forward.\n",
            "\n",
            "The first step is to understand the importance of your work\n",
            "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
            "\n",
            "The following text 'spam'? Answer with 'yes' or\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "torch.manual_seed(123)\n",
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(\n",
        "    in_features=BASE_CONFIG[\"emb_dim\"],\n",
        "    out_features=num_classes\n",
        ")\n",
        "\n",
        "# Unfreeze final transformer block\n",
        "for param in model.trf_blocks[-1].parameters():\n",
        "  param.requires_grad=True\n",
        "\n",
        "for param in model.final_norm.parameters():\n",
        "  param.requires_grad=True"
      ],
      "metadata": {
        "id": "IWiXLLDgQq_5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(\"Do you have time\")\n",
        "inputs = torch.tensor(inputs).unsqueeze(0)\n",
        "print(\"Inputs: \", inputs)\n",
        "print(\"Input Dimensions: \", inputs.shape)\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model(inputs)\n",
        "  print(\"Outputs: \", outputs)\n",
        "  print(\"Output Dimensions: \", outputs.shape)\n",
        "\n",
        "\n",
        "last_output_tokens = outputs[:,-1,:]\n",
        "print(\"last output tokens:\", last_output_tokens)\n",
        "\n",
        "probas = torch.softmax(last_output_tokens, dim=-1)\n",
        "print(\"Probas: \", probas)\n",
        "label = torch.argmax(probas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "collapsed": true,
        "id": "RTFJeQ9sRZI_",
        "outputId": "14e299ae-f64c-49e7-a39a-7dc6ae09977c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:  tensor([[5211,  345,  423,  640]])\n",
            "Input Dimensions:  torch.Size([1, 4])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2510009820.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Outputs: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output Dimensions: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/previous_chapters.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in_idx)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mtok_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mpos_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_embeds\u001b[0m  \u001b[0;31m# Shape [batch_size, num_tokens, emb_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "  model.eval()\n",
        "  correct_predictions, total_predictions = 0, 0\n",
        "  if num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    if batch_idx < num_batches:\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        logits = model(inputs)[:,-1,:]\n",
        "      predicted_labels = torch.argmax(logits, dim=-1)\n",
        "      total_predictions += predicted_labels.shape[0]\n",
        "      correct_predictions += (predicted_labels == targets).sum().item()\n",
        "    else:\n",
        "      break\n",
        "  return correct_predictions / total_predictions"
      ],
      "metadata": {
        "id": "LC_LH7VNZdOY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device, 10)\n",
        "val_accuracy = calc_accuracy_loader(validation_loader, model, device, 10)\n",
        "\n",
        "test_accuracy = calc_accuracy_loader(\n",
        " test_loader, model, device, num_batches=10\n",
        ")\n",
        "\n",
        "# Not fine-tuned. Basically random guessing ~50%\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHBozOCuaTc4",
        "outputId": "6ca548da-cbb4-4c18-e790-0e61e4add91f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 46.25%\n",
            "Validation accuracy: 45.00%\n",
            "Test accuracy: 48.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "  logits = model(input_batch)[:,-1,:]\n",
        "  loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "  return loss\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "  loss = 0\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if i >= num_batches:\n",
        "      break\n",
        "    loss += calc_loss_batch(input_batch, target_batch, model, device).item()\n",
        "  return loss / num_batches"
      ],
      "metadata": {
        "id": "vpwzbf3KdFbg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "  val_loss = calc_loss_loader(validation_loader, model, device, num_batches=5)\n",
        "  test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
        "print(f\"Training loss: {train_loss:.3f}\")\n",
        "print(f\"Validation loss: {val_loss:.3f}\")\n",
        "print(f\"Test loss: {test_loss:.3f}\")\n",
        "\n",
        "# Don't expect losses to be the same. Batches are shuffled each time and only using 5 batches, far less than the dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42N59kVzep9W",
        "outputId": "6f6227d0-4a23-4b74-c54c-a9e933c65651"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 2.194\n",
            "Validation loss: 2.583\n",
            "Test loss: 2.322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, eval_iter)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, eval_iter)\n",
        "  model.train()\n",
        "  return train_loss, val_loss\n",
        "\n",
        "def train_classifier_simple(model, train_loader, val_loader, optimizer,\n",
        "                            device, num_epochs, eval_freq, eval_iter):\n",
        "  train_losses, val_losses, train_acc, val_acc = [],[],[],[]\n",
        "  examples_seen, global_step = 0, -1\n",
        "  model.to(device)\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      examples_seen += input_batch.shape[0]\n",
        "      global_step += 1\n",
        "\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f\"Ep {epoch+1} (Step {global_step:06d}): Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "    train_accuracy = calc_accuracy_loader(\n",
        "    train_loader, model, device, num_batches=eval_iter)\n",
        "    val_accuracy = calc_accuracy_loader(\n",
        "    val_loader, model, device, num_batches=eval_iter)\n",
        "    print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "    print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "    train_acc.append(train_accuracy)\n",
        "    val_acc.append(val_accuracy)\n",
        "  return train_losses, val_losses, train_acc, val_acc, examples_seen"
      ],
      "metadata": {
        "id": "Q8DWIAylhEId"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "num_epochs = 5\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
        " train_classifier_simple(\n",
        " model, train_loader, validation_loader, optimizer, device,\n",
        " num_epochs=num_epochs, eval_freq=50,\n",
        " eval_iter=5\n",
        " )\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU_l7uFUk2h1",
        "outputId": "08a4e683-f249-4001-ae53-dc41294d3a1c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392\n",
            "Ep 1 (Step 000050): Train loss 0.617, Val loss 0.637\n",
            "Ep 1 (Step 000100): Train loss 0.523, Val loss 0.557\n",
            "Training accuracy: 70.00% | Validation accuracy: 72.50%\n",
            "Ep 2 (Step 000150): Train loss 0.561, Val loss 0.489\n",
            "Ep 2 (Step 000200): Train loss 0.419, Val loss 0.397\n",
            "Ep 2 (Step 000250): Train loss 0.409, Val loss 0.353\n",
            "Training accuracy: 82.50% | Validation accuracy: 85.00%\n",
            "Ep 3 (Step 000300): Train loss 0.333, Val loss 0.320\n",
            "Ep 3 (Step 000350): Train loss 0.340, Val loss 0.306\n",
            "Training accuracy: 90.00% | Validation accuracy: 90.00%\n",
            "Ep 4 (Step 000400): Train loss 0.136, Val loss 0.200\n",
            "Ep 4 (Step 000450): Train loss 0.153, Val loss 0.132\n",
            "Ep 4 (Step 000500): Train loss 0.222, Val loss 0.137\n",
            "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
            "Ep 5 (Step 000550): Train loss 0.207, Val loss 0.143\n",
            "Ep 5 (Step 000600): Train loss 0.083, Val loss 0.074\n",
            "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
            "Training completed in 1.00 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "val_accuracy = calc_accuracy_loader(validation_loader, model, device)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA_zHBI6mcrr",
        "outputId": "d893a88f-ac81-4537-ba6e-622faf35a13d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 97.21%\n",
            "Validation accuracy: 97.32%\n",
            "Test accuracy: 95.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\"model_state\" : model.state_dict(),\n",
        "            \"optimizer_state\" : optimizer.state_dict()}, \"gpt_classifier.pth\")"
      ],
      "metadata": {
        "id": "1hC_9cbepLMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model2 = GPTModel(BASE_CONFIG)\n",
        "model2.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"],\n",
        "    out_features=2)\n",
        "checkpoint = torch.load(\"gpt_classifier.pth\", map_location=device)\n",
        "model2.load_state_dict(checkpoint[\"model_state\"])\n",
        "optimizer2 = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "optimizer2.load_state_dict(checkpoint[\"optimizer_state\"])"
      ],
      "metadata": {
        "id": "eJOWe3cnqirz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test code\n",
        "model2.eval()\n",
        "text1 = \"Every effort moves you\"\n",
        "token_ids = generate_text_simple(model=model2, idx=text_to_token_ids(text1, tokenizer),\n",
        "                                 max_new_tokens=1,\n",
        "                                 context_size=BASE_CONFIG[\"context_length\"])\n",
        "print(token_ids)\n",
        "print(token_ids_to_text(token_ids, tokenizer))\n",
        "# Shouldn't print out good stuff. Since argmax from generate_text_simple only return 0 or 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJsR4LAgmlRq",
        "outputId": "38894a1a-41ba-4019-e384-b95fa03456ca"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345,    1]])\n",
            "Every effort moves you\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
        "  model.eval()\n",
        "\n",
        "  input_ids = tokenizer.encode(text)\n",
        "  supported_context_length = model.pos_emb.weight.shape[0]\n",
        "\n",
        "  max_len = min(max_length, supported_context_length) if max_length is not None else supported_context_length\n",
        "  input_ids = input_ids[:max_len]\n",
        "\n",
        "  input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
        "  input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    logits = model(input_ids)[:,-1,:]\n",
        "  predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "  return \"spam\" if predicted_label == 1 else \"not spam\"\n"
      ],
      "metadata": {
        "id": "4oO0r_rChhcH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = (\n",
        " \"You are a winner you have been specially\"\n",
        " \" selected to receive $1000 cash or a $2000 award.\"\n",
        ")\n",
        "print(classify_review(text_1, model2, tokenizer, device,\n",
        "                      max_length=train_dataset.max_length))\n",
        "\n",
        "text_2 = (\"Hey, just wanted to check if we're still on\"\n",
        " \" for dinner tonight? Let me know!\")\n",
        "print(classify_review(text_2, model2, tokenizer,\n",
        "                      device, max_length=train_dataset.max_length))\n",
        "\n",
        "text_3 = (\"Every effort moves you\")\n",
        "print(classify_review(text_3, model2, tokenizer, device,\n",
        "                      max_length=train_dataset.max_length))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFirlp2Gm0M8",
        "outputId": "129ec397-d71d-4be5-e8e7-4d524123c41b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n",
            "not spam\n",
            "not spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy_loader_var(data_loader, model, device, num_batches=None, token_idx=-1):\n",
        "  model.eval()\n",
        "  correct_predictions, total_predictions = 0, 0\n",
        "  if num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    if batch_idx < num_batches:\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        logits = model(inputs)[:,token_idx,:]\n",
        "      predicted_labels = torch.argmax(logits, dim=-1)\n",
        "      total_predictions += predicted_labels.shape[0]\n",
        "      correct_predictions += (predicted_labels == targets).sum().item()\n",
        "    else:\n",
        "      break\n",
        "  return correct_predictions / total_predictions\n",
        "\n",
        "\n",
        "def calc_loss_batch_var(input_batch, target_batch, model, device, token_idx=-1):\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "  logits = model(input_batch)[:,token_idx,:]\n",
        "  loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "  return loss\n",
        "\n",
        "def calc_loss_loader_var(data_loader, model, device, num_batches=None, token_idx=-1):\n",
        "  loss = 0\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if i >= num_batches:\n",
        "      break\n",
        "    loss += calc_loss_batch_var(input_batch, target_batch, model, device, token_idx).item()\n",
        "  return loss / num_batches\n",
        "\n",
        "def evaluate_model_var(model, train_loader, val_loader, device, eval_iter, token_idx):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    train_loss = calc_loss_loader_var(train_loader, model, device, eval_iter, token_idx)\n",
        "    val_loss = calc_loss_loader_var(val_loader, model, device, eval_iter, token_idx)\n",
        "  model.train()\n",
        "  return train_loss, val_loss\n",
        "\n",
        "def train_classifier_simple_var(model, train_loader, val_loader, optimizer,\n",
        "                            device, num_epochs, eval_freq, eval_iter, token_idx=-1):\n",
        "  train_losses, val_losses, train_acc, val_acc = [],[],[],[]\n",
        "  examples_seen, global_step = 0, -1\n",
        "  model.to(device)\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = calc_loss_batch_var(input_batch, target_batch, model, device, token_idx)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      examples_seen += input_batch.shape[0]\n",
        "      global_step += 1\n",
        "\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model_var(model, train_loader, val_loader, device, eval_iter, token_idx)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f\"Ep {epoch+1} (Step {global_step:06d}): Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "    train_accuracy = calc_accuracy_loader_var(\n",
        "    train_loader, model, device, num_batches=eval_iter, token_idx=token_idx)\n",
        "    val_accuracy = calc_accuracy_loader_var(\n",
        "    val_loader, model, device, num_batches=eval_iter, token_idx=token_idx)\n",
        "    print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "    print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "    train_acc.append(train_accuracy)\n",
        "    val_acc.append(val_accuracy)\n",
        "  return train_losses, val_losses, train_acc, val_acc, examples_seen"
      ],
      "metadata": {
        "id": "EbDvZfm_u6vF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Different hyperparameter tuning\n",
        "num_workers3 = 0\n",
        "batch_size3 = 8\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model with padding the input to maximum context_length\n",
        "train_dataset3= SpamDataset(csv_file=\"train.tsv\", tokenizer=tokenizer, max_length=None)\n",
        "#print(train_dataset3.max_length)\n",
        "validation_dataset3 = SpamDataset(\"validation.tsv\", tokenizer, max_length=train_dataset3.max_length)\n",
        "#print(validation_dataset3.max_length)\n",
        "# NEED max_length = train_dataset3.max_length to ENSURE stable training!\n",
        "test_dataset3 = SpamDataset(\"test.tsv\", tokenizer, max_length=train_dataset3.max_length)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_loader3 = DataLoader(dataset=train_dataset3, batch_size=batch_size3,\n",
        "                          shuffle=True, drop_last=True, num_workers=num_workers3)\n",
        "val_loader3 = DataLoader(dataset=validation_dataset3, batch_size=batch_size3,\n",
        "                        shuffle=False, drop_last=False, num_workers=num_workers3)\n",
        "\n",
        "test_loader3 = DataLoader(dataset=test_dataset3, batch_size=batch_size3,\n",
        "                         shuffle=False, drop_last=False, num_workers=num_workers3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model3 = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model3, params)\n",
        "torch.manual_seed(123)\n",
        "model3.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"],\n",
        "    out_features=2)\n",
        "\n",
        "# Freeze parameters\n",
        "for param in model3.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Unfreeze out_head and last transformer block and final_norm\n",
        "for param in model3.out_head.parameters():\n",
        "  param.requires_grad = True\n",
        "for param in model3.trf_blocks[-1].parameters():\n",
        "  param.requires_grad = True\n",
        "for param in model3.final_norm.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "\n",
        "# set token_idx = 0, it performs much worse. In fact it overfits training data after epoch2 and beyond\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer3 = torch.optim.AdamW(model3.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "num_epochs = 5\n",
        "train_losses3, val_losses3, train_accs3, val_accs3, examples_seen3 = \\\n",
        " train_classifier_simple_var(\n",
        " model3, train_loader3, val_loader3, optimizer3, device,\n",
        " num_epochs=num_epochs, eval_freq=50,\n",
        " eval_iter=5, token_idx=10\n",
        " )\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmooVMAyqHv5",
        "outputId": "1ffe70fa-069f-47c2-a9a1-ff376e891469"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 3.228, Val loss 4.034\n",
            "Ep 1 (Step 000050): Train loss 1.184, Val loss 1.403\n",
            "Ep 1 (Step 000100): Train loss 0.448, Val loss 0.628\n",
            "Training accuracy: 77.50% | Validation accuracy: 70.00%\n",
            "Ep 2 (Step 000150): Train loss 0.446, Val loss 0.535\n",
            "Ep 2 (Step 000200): Train loss 0.261, Val loss 0.438\n",
            "Ep 2 (Step 000250): Train loss 0.344, Val loss 0.389\n",
            "Training accuracy: 85.00% | Validation accuracy: 77.50%\n",
            "Ep 3 (Step 000300): Train loss 0.238, Val loss 0.350\n",
            "Ep 3 (Step 000350): Train loss 0.309, Val loss 0.317\n",
            "Training accuracy: 90.00% | Validation accuracy: 87.50%\n",
            "Ep 4 (Step 000400): Train loss 0.255, Val loss 0.289\n",
            "Ep 4 (Step 000450): Train loss 0.237, Val loss 0.274\n",
            "Ep 4 (Step 000500): Train loss 0.236, Val loss 0.258\n",
            "Training accuracy: 97.50% | Validation accuracy: 87.50%\n",
            "Ep 5 (Step 000550): Train loss 0.081, Val loss 0.248\n",
            "Ep 5 (Step 000600): Train loss 0.214, Val loss 0.248\n",
            "Training accuracy: 100.00% | Validation accuracy: 90.00%\n",
            "Training completed in 1.00 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice depending on which token_idx the model is trained on, we need to stay consistent\n",
        "# even if trained by taking the last token of the sequence to predict class, using different\n",
        "# a different index doesn't generalize well. However, the further back we go, the more we can use earlier\n",
        "# tokens to generalize. This makes sense intuitevely as going \"earlier\" than what we trained\n",
        "# we should expect those tokens to still have some info but less\n",
        "'''\n",
        "For example say we trained using idx=10 but then evaluate using idx=11 or 12. You can see accuracy drops\n",
        "significantly, but a few percentage points even by just changing the idx by +=1. This is because the model\n",
        "has not yet learned how to incorporate later indices due to casual attention\n",
        "\n",
        "'''\n",
        "\n",
        "# hyperparameters\n",
        "token_idx3=100\n",
        "token_idx_org=-100\n",
        "\n",
        "\n",
        "train_accuracy3 = calc_accuracy_loader_var(train_loader3, model3, device, token_idx=token_idx3)\n",
        "val_accuracy3 = calc_accuracy_loader_var(val_loader3, model3, device, token_idx=token_idx3)\n",
        "test_accuracy3 = calc_accuracy_loader_var(test_loader3, model3, device, token_idx=token_idx3)\n",
        "\n",
        "print(f\"Training accuracy3: {train_accuracy3*100:.2f}%\")\n",
        "print(f\"Validation accuracy3: {val_accuracy3*100:.2f}%\")\n",
        "print(f\"Test accuracy3: {test_accuracy3*100:.2f}%\")\n",
        "\n",
        "\n",
        "train_accuracy = calc_accuracy_loader_var(train_loader, model, device, token_idx=token_idx_org)\n",
        "val_accuracy = calc_accuracy_loader_var(validation_loader, model, device, token_idx=token_idx_org)\n",
        "test_accuracy = calc_accuracy_loader_var(test_loader, model, device, token_idx=token_idx_org)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoQDK4Kz2PEw",
        "outputId": "47047f0e-3928-41e1-8457-5acce2810237"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy3: 50.58%\n",
            "Validation accuracy3: 46.98%\n",
            "Test accuracy3: 49.67%\n",
            "Training accuracy: 64.33%\n",
            "Validation accuracy: 62.42%\n",
            "Test accuracy: 62.33%\n"
          ]
        }
      ]
    }
  ]
}