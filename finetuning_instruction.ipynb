{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORw8UZm08DGM4mxmEenDc9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlinapp/proofLLM/blob/main/finetuning_instruction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui9AcMuM7nO2",
        "outputId": "a03c16c1-7375-4a89-b657-faf382a24963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 1100\n",
            "dict_items([('instruction', 'Evaluate the following phrase by transforming it into the spelling given.'), ('input', 'freind --> friend'), ('output', 'The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".')])\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import urllib\n",
        "\n",
        "def download_and_load_file(file_path, url):\n",
        "  if not os.path.exists(file_path):\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "      text_data = response.read().decode('utf-8')\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(text_data)\n",
        "\n",
        "  with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "  return data\n",
        "\n",
        "file_path = \"instruction-data.json\"\n",
        "url = url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "data = download_and_load_file(file_path, url)\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Number of entries:\", len(data))\n",
        "print(data[0].items())\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3lM5yP7TR3P",
        "outputId": "61ccf135-6036-4fc8-b7ad-6bf947d2ad0c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'Suggest a more formal synonym for \"happy.\"', 'input': '', 'output': 'A more formal synonym for \"happy\" is \"content.\"'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(entry):\n",
        "  instruction_text = (\n",
        "  f\"Below is an instruction that describes a task. \"\n",
        "  f\"Write a response that appropriately completes the request.\"\n",
        "  f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "  )\n",
        "\n",
        "  input_text = (\n",
        "  f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "  )\n",
        "  return instruction_text + input_text"
      ],
      "metadata": {
        "id": "JmChvK0R_2CO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = format_input(data[50])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
        "print(model_input + desired_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbDBGy51BCC2",
        "outputId": "c714d955-1024-42a6-a6dc-d2f256120103"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Identify the correct spelling of the following word.\n",
            "\n",
            "### Input:\n",
            "Ocassion\n",
            "\n",
            "### Response:\n",
            "The correct spelling is 'Occasion.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_portion = int(len(data) * 0.85)   # 85% for training\n",
        "test_portion = int(len(data) * 0.1)     # 10% for test\n",
        "val_portion = len(data) - train_portion - test_portion    # 5 % for validation\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]\n",
        "\n",
        "\n",
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6Y0SCHZBsdf",
        "outputId": "2c911372-a78e-49da-a137-e2a269e77359"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length: 935\n",
            "Validation set length: 55\n",
            "Test set length: 110\n",
            "{'instruction': 'Evaluate the following phrase by transforming it into the spelling given.', 'input': 'freind --> friend', 'output': 'The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class InstructionDataSet(Dataset):\n",
        "  def __init__(self, data, tokenizer):\n",
        "    self.data = data\n",
        "    self.encoded_texts = []\n",
        "    for entry in data:\n",
        "      instruction_plus_input = format_input(entry)\n",
        "      response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "      full_text = instruction_plus_input + response_text\n",
        "      self.encoded_texts.append(tokenizer.encode(full_text))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.encoded_texts[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.encoded_texts)\n",
        "\n"
      ],
      "metadata": {
        "id": "4aNnoRGWZPR9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU7ehWg0dcxQ",
        "outputId": "6c0236d9-f64c-45f0-90b6-9506feae332b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_draft1(batch, device=\"cpu\", pad_token_id=50256):\n",
        "  batch_max_length = max(len(item) + 1 for item in batch)\n",
        "  inputs_lst = []\n",
        "\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "    new_item += [pad_token_id]\n",
        "    padded = (new_item + [pad_token_id] * (batch_max_length - len(new_item)))\n",
        "\n",
        "    inputs = torch.tensor(padded[:-1])\n",
        "    inputs_lst.append(inputs)\n",
        "\n",
        "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "  return inputs_tensor\n",
        "\n",
        "def custom_collate_draft2(batch, device=\"cpu\", pad_token_id=50256):\n",
        "  batch_max_length = max(len(item) + 1 for item in batch)\n",
        "  inputs_lst, targets_lst = [], []\n",
        "\n",
        "\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "    new_item += [pad_token_id]\n",
        "    padded = (new_item + [pad_token_id] * (batch_max_length - len(new_item)))\n",
        "\n",
        "    inputs = torch.tensor(padded[:-1])\n",
        "    targets = torch.tensor(padded[1:])\n",
        "    inputs_lst.append(inputs)\n",
        "    targets_lst.append(targets)\n",
        "\n",
        "\n",
        "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "  targets_tensor = torch.stack(targets_lst).to(device)\n",
        "  return inputs_tensor, targets_tensor\n",
        "\n",
        "\n",
        "# use ignore_idx = -100 because built in PyTorch nn.crossentropy loss ignores targets with target_label=-100\n",
        "def custom_collate_fn(batch, ignore_idx=-100, allowed_max_length=None, device=\"cpu\", pad_token_id=50256):\n",
        "  batch_max_length = max(len(item) + 1 for item in batch)\n",
        "  inputs_lst, targets_lst = [], []\n",
        "\n",
        "\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "    new_item += [pad_token_id]\n",
        "    padded = (new_item + [pad_token_id] * (batch_max_length - len(new_item)))\n",
        "\n",
        "    inputs = torch.tensor(padded[:-1])\n",
        "    targets = torch.tensor(padded[1:])\n",
        "\n",
        "    mask = targets == pad_token_id\n",
        "    indices = torch.nonzero(mask).squeeze()\n",
        "    if indices.numel() > 1:\n",
        "      targets[indices[1:]] = ignore_idx   # using targets[[5,6,7]] where [5,6,7] could be list or tensor or tuple and sets tho elements to ignore_idx\n",
        "\n",
        "    inputs_lst.append(inputs)\n",
        "    targets_lst.append(targets)\n",
        "\n",
        "\n",
        "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "  targets_tensor = torch.stack(targets_lst).to(device)\n",
        "  return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "hROTcgl1d5Gn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing custom_collate_draft1 function\n",
        "\n",
        "inputs_1 = [0,1,2,3,4]\n",
        "inputs_2 = [5,6]\n",
        "inputs_3 = [7,8,9]\n",
        "batch = (inputs_1, inputs_2, inputs_3)\n",
        "\n",
        "print(custom_collate_draft1(batch))\n",
        "\n",
        "inputs, targets = custom_collate_draft2(batch)\n",
        "print(inputs)\n",
        "print(targets)\n",
        "\n",
        "inputs, targets = custom_collate_fn(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiAI3lMSr7G_",
        "outputId": "aaf08e0e-4b85-4c89-abc8-f358c876a6f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256, 50256, 50256, 50256],\n",
            "        [    8,     9, 50256, 50256, 50256]])\n",
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataSet(train_data, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                          collate_fn=customized_collate_fn,\n",
        "                          drop_last=True,\n",
        "                          num_workers=num_workers)\n",
        "\n",
        "val_dataset = InstructionDataSet(val_data, tokenizer)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
        "                        collate_fn=customized_collate_fn,\n",
        "                        shuffle=False,\n",
        "                        drop_last=False,\n",
        "                        num_workers=num_workers)\n",
        "\n",
        "\n",
        "test_dataset = InstructionDataSet(test_data, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                        collate_fn=customized_collate_fn,\n",
        "                        shuffle=False,\n",
        "                        drop_last=False,\n",
        "                        num_workers=num_workers)\n",
        "print(\"Train loader:\")\n",
        "for inputs, targets in train_loader:\n",
        "  print(inputs.shape, targets.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EE5FZZOc2-jy",
        "outputId": "b660d61e-9358-42ce-b165-d1418730f97f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Train loader:\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 57]) torch.Size([8, 57])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "Download boilerplate code from gpt_download.py\n",
        "\n",
        "'''\n",
        "import importlib\n",
        "import urllib.request\n",
        "url = (\n",
        " \"https://raw.githubusercontent.com/rasbt/\"\n",
        " \"LLMs-from-scratch/main/ch05/\"\n",
        " \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split('/')[-1]\n",
        "file_name, _ = urllib.request.urlretrieve(url, filename)\n",
        "from gpt_download import download_and_load_gpt2\n",
        "import previous_chapters\n",
        "importlib.reload(previous_chapters)\n",
        "from previous_chapters import *\n"
      ],
      "metadata": {
        "id": "_9kiz7DYHE6P"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_CONFIG = {\n",
        " \"vocab_size\": 50257, # Vocabulary size\n",
        " \"context_length\": 1024, # Context length\n",
        " \"drop_rate\": 0.0, # Dropout rate\n",
        " \"qkv_bias\": True # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        " \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        " \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        " \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        " \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(\n",
        " model_size=model_size,\n",
        " models_dir=\"gpt2\"\n",
        ")\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkxiY7lGHNLl",
        "outputId": "52cd80be-b70a-4dba-fecc-2c1bf202b03c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "input_text = format_input(val_data[0])\n",
        "print(input_text)\n",
        "\n",
        "token_ids = generate(model=model, idx=text_to_token_ids(input_text, tokenizer),\n",
        "                     max_new_tokens=35,\n",
        "                     context_size=BASE_CONFIG['context_length'],\n",
        "                     eos_id=50256)\n",
        "generated_text = token_ids_to_text(token_ids, tokenizer);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfU2NWdKHuAr",
        "outputId": "92f9781e-0e7e-42c1-e8bc-b7c10436b7bf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text)\n",
        "response_text = generated_text[len(input_text):].strip()\n",
        "print(\"==========================\")\n",
        "print(response_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoDfkD69Kgqq",
        "outputId": "a7024c4d-f21e-4a95-efd2-fe500a23bb73"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
            "\n",
            "### Response:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "Convert the active sentence to passive: 'The chef cooks the\n",
            "==========================\n",
            "### Response:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "Convert the active sentence to passive: 'The chef cooks the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model, device=device, num_batches=5)\n",
        "  val_loss = calc_loss_loader(val_loader, model, device=device, num_batches=5)\n",
        "\n",
        "print(f\"Train loss: {train_loss:.4f}\")\n",
        "print(f\"Validation loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeB021qZLU21",
        "outputId": "598ba08b-2e4f-44d1-f3f8-aaf58b3c74e0"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.8259\n",
            "Validation loss: 3.7619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "num_epochs = 2\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(model=model,\n",
        "                                                           train_loader=train_loader,\n",
        "                                                           val_loader=val_loader,\n",
        "                                                           optimizer=optimizer,\n",
        "                                                           device=device,\n",
        "                                                           num_epochs=num_epochs,\n",
        "                                                           eval_freq=5,\n",
        "                                                           eval_iter=5,\n",
        "                                                           start_context=format_input(val_data[0]),\n",
        "                                                           tokenizer=tokenizer)\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = (end_time - start_time) / 60\n",
        "print(f\"Training complete in: {execution_time} minutes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfG_72zLLsTb",
        "outputId": "0103f0c2-0e6d-4700-db1e-28334443568b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
            "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.103\n",
            "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.944\n",
            "Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906\n",
            "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
            "Ep 1 (Step 000025): Train loss 0.754, Val loss 0.859\n",
            "Ep 1 (Step 000030): Train loss 0.799, Val loss 0.836\n",
            "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808\n",
            "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
            "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.789\n",
            "Ep 1 (Step 000050): Train loss 0.663, Val loss 0.783\n",
            "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.763\n",
            "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
            "Ep 1 (Step 000065): Train loss 0.653, Val loss 0.735\n",
            "Ep 1 (Step 000070): Train loss 0.533, Val loss 0.729\n",
            "Ep 1 (Step 000075): Train loss 0.568, Val loss 0.729\n",
            "Ep 1 (Step 000080): Train loss 0.604, Val loss 0.725\n",
            "Ep 1 (Step 000085): Train loss 0.509, Val loss 0.710\n",
            "Ep 1 (Step 000090): Train loss 0.563, Val loss 0.691\n",
            "Ep 1 (Step 000095): Train loss 0.502, Val loss 0.681\n",
            "Ep 1 (Step 000100): Train loss 0.504, Val loss 0.677\n",
            "Ep 1 (Step 000105): Train loss 0.565, Val loss 0.670\n",
            "Ep 1 (Step 000110): Train loss 0.554, Val loss 0.666\n",
            "Ep 1 (Step 000115): Train loss 0.509, Val loss 0.663\n",
            "task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
            "\n",
            "### Response:\n",
            "The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
            "\n",
            "### Response:\n",
            "The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Ep 2 (Step 000120): Train loss 0.435, Val loss 0.671\n",
            "Ep 2 (Step 000125): Train loss 0.451, Val loss 0.686\n",
            "Ep 2 (Step 000130): Train loss 0.447, Val loss 0.682\n",
            "Ep 2 (Step 000135): Train loss 0.405, Val loss 0.681\n",
            "Ep 2 (Step 000140): Train loss 0.410, Val loss 0.681\n",
            "Ep 2 (Step 000145): Train loss 0.369, Val loss 0.681\n",
            "Ep 2 (Step 000150): Train loss 0.382, Val loss 0.675\n",
            "Ep 2 (Step 000155): Train loss 0.414, Val loss 0.675\n",
            "Ep 2 (Step 000160): Train loss 0.412, Val loss 0.684\n",
            "Ep 2 (Step 000165): Train loss 0.379, Val loss 0.686\n",
            "Ep 2 (Step 000170): Train loss 0.322, Val loss 0.680\n",
            "Ep 2 (Step 000175): Train loss 0.337, Val loss 0.668\n",
            "Ep 2 (Step 000180): Train loss 0.392, Val loss 0.656\n",
            "Ep 2 (Step 000185): Train loss 0.414, Val loss 0.657\n",
            "Ep 2 (Step 000190): Train loss 0.340, Val loss 0.648\n",
            "Ep 2 (Step 000195): Train loss 0.328, Val loss 0.634\n",
            "Ep 2 (Step 000200): Train loss 0.309, Val loss 0.634\n",
            "Ep 2 (Step 000205): Train loss 0.353, Val loss 0.631\n",
            "Ep 2 (Step 000210): Train loss 0.362, Val loss 0.631\n",
            "Ep 2 (Step 000215): Train loss 0.393, Val loss 0.634\n",
            "Ep 2 (Step 000220): Train loss 0.297, Val loss 0.644\n",
            "Ep 2 (Step 000225): Train loss 0.341, Val loss 0.658\n",
            "Ep 2 (Step 000230): Train loss 0.294, Val loss 0.656\n",
            "task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
            "\n",
            "### Response:\n",
            "The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the capital of the United Kingdom?\n",
            "\n",
            "### Response:\n",
            "The capital of the United Kingdom is London.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the capital of the Netherlands?\n",
            "Training complete in: 3.2179026643435162 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "for entry in test_data[:3]:\n",
        "  input_text = format_input(entry)    # format_input takes in a dict\n",
        "  token_ids = generate(model=model,\n",
        "                       idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "                       max_new_tokens=256,\n",
        "                       context_size=BASE_CONFIG['context_length'],\n",
        "                       eos_id=50256)\n",
        "  generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "  response_text = (generated_text[len(input_text):]\n",
        "                   .replace(\"### Response:\", \"\")\n",
        "                   .strip())\n",
        "  print(input_text)\n",
        "  print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "  print(f\"\\nModel's PURE RESPONSE:\\n>> {generated_text.strip()}\")\n",
        "  print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "  print(\"-------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ar2v40xQVns",
        "outputId": "c5b6d2f1-b998-4540-c88b-3c9f81476e84"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the sentence using a simile.\n",
            "\n",
            "### Input:\n",
            "The car is very fast.\n",
            "\n",
            "Correct response:\n",
            ">> The car is as fast as lightning.\n",
            "\n",
            "Model's PURE RESPONSE:\n",
            ">> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the sentence using a simile.\n",
            "\n",
            "### Input:\n",
            "The car is very fast.\n",
            "\n",
            "### Response:\n",
            "The car is as fast as a bullet.\n",
            "\n",
            "Model response:\n",
            ">> The car is as fast as a bullet.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What type of cloud is typically associated with thunderstorms?\n",
            "\n",
            "Correct response:\n",
            ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
            "\n",
            "Model's PURE RESPONSE:\n",
            ">> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What type of cloud is typically associated with thunderstorms?\n",
            "\n",
            "### Response:\n",
            "The type of cloud associated with thunderstorms is a cumulus cloud.\n",
            "\n",
            "Model response:\n",
            ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name the author of 'Pride and Prejudice'.\n",
            "\n",
            "Correct response:\n",
            ">> Jane Austen.\n",
            "\n",
            "Model's PURE RESPONSE:\n",
            ">> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name the author of 'Pride and Prejudice'.\n",
            "\n",
            "### Response:\n",
            "The author of 'Pride and Prejudice' is Jane Austen.\n",
            "\n",
            "Model response:\n",
            ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "  input_text = format_input(entry)\n",
        "\n",
        "  token_ids = generate(model=model,\n",
        "                       idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "                       max_new_tokens=256,\n",
        "                       context_size=BASE_CONFIG['context_length'],\n",
        "                       eos_id=50256)\n",
        "  generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "  response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
        "  test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
        " json.dump(test_data, file, indent=4)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc0ct6wzWg7V",
        "outputId": "bb5eb744-6683-40ed-edbd-29b588b737ac"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110/110 [01:13<00:00,  1.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
        "torch.save(model.state_dict(), file_name)\n",
        "print(f\"Model saved as {file_name}\")\n",
        "\n",
        "# Load model via\n",
        "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF-pmzcMgIQZ",
        "outputId": "4246fc58-1983-431c-a502-e484cee339e4"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as gpt2-medium355M-sft.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import previous_chapters\n",
        "importlib.reload(previous_chapters)\n",
        "from previous_chapters import *\n",
        "\n",
        "# WHAT HPAPENS WHEN YOU DONTN STOP AT EOS. The model seems to hallucinate\n",
        "# afterwards. Most liekly because it was not trained well after EOS\n",
        "#print(text_to_token_ids(format_input(val_data[0]), tokenizer).shape)\n",
        "s = generate_and_print_sample(model, tokenizer, device=device,\n",
        "                              start_context=format_input(val_data[1]))\n",
        "\n",
        "print(\"HELLEOEAIJILLAKSJDLKSADJ\")\n",
        "print(format_input(val_data[1]))\n",
        "print(\"========\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9288GO4ZiKQ",
        "outputId": "489b522a-b7bb-43fc-d05d-da7e4251230c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "### Response:\n",
            "Dance is a verb.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the capital of the United Kingdom?\n",
            "\n",
            "### Response:\n",
            "The capital of the United Kingdom is London.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the capital of the Netherlands?\n",
            "\n",
            "### Response\n",
            "HELLEOEAIJILLAKSJDLKSADJ\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Classify an input string as either a noun or a verb.\n",
            "\n",
            "### Input:\n",
            "Dance\n",
            "========\n"
          ]
        }
      ]
    }
  ]
}