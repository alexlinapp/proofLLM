{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN99W7S0rgCVkbqLtBeH4du",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlinapp/proofLLM/blob/main/finetuning_reasoning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "52564eebf3d1478b97576727bb8a7d6c",
            "9aa06921939149b48fa5f8dddc4abac6",
            "79d1af284c48437f8914c5cac3bba7e5",
            "6b6d6c5eea5447ff846541c4cd292380",
            "a3844c67d918442f80efaef291ff8f99",
            "1f2d9e6bae80435d96072ba08f25fca5",
            "d21554f060bb451e9eff6a7942aff2cb",
            "20826ee3c4784e8d990eac103413938d",
            "b0367fd0f82644dfa3a4fe8a26cceaf8",
            "014de2efbc3f4495998bf65927cc3f1f",
            "20b52822b2de4a3ab8796b5e2380c2dd",
            "50c7ccd7f76f4a6b8a1326529b9f5ded",
            "ae68481b3bc74385a244b8269c4bc15d",
            "3e0dfe1232264260ba4979b103c437d3",
            "d79c345b45e547bb8aabdcacaee554ba",
            "5349766f99f64197b7c4beceaacbe0d6",
            "0abf58a07fe248f98b3ed1bbd8ed15eb"
          ]
        },
        "id": "xlw9WpSo0SDd",
        "outputId": "b8fa7ee8-6ce6-4686-f774-c55c80b916c5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52564eebf3d1478b97576727bb8a7d6c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Authentication. Make sure you have authentication token as well as access to the gated repo\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login()\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "# model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
        "# pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\" : torch.bfloat16}, device_map=\"auto\")\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.lm_head)\n",
        "#x = model(torch.tensor([[1, 2, 3]]))\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOvpWlMu8SUX",
        "outputId": "f2bcdde9-8b1d-454c-de27-cefd90595377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=768, out_features=50257, bias=False)\n",
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# config = model.config\n",
        "# output = model(torch.tensor([[1, 2, 3]]))\n",
        "# print(output.logits)\n",
        "# for key, value in model.named_parameters():\n",
        "#   print(key)\n",
        "for params in model.parameters():\n",
        "  params.requires_grad = False\n",
        "for params in model.lm_head.parameters():\n",
        "  params.requires_grad = True\n",
        "for params in model.transformer.h[11].parameters():\n",
        "  params.requires_grad = True\n",
        "print(\"Trainable Parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "id": "c_RNfzYOTq_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b6fcd3-87a7-48a0-a0f0-8cada3dbad8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable Parameters: 45685248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# testing data\n",
        "dataset = load_dataset(\"cais/mmlu\", \"abstract_algebra\")\n",
        "print(dataset.keys())\n",
        "print(type(dataset[\"test\"]))\n",
        "print(len(dataset[\"test\"]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325,
          "referenced_widgets": [
            "b3b3ba0bda794d5bbc8b5188c50f6c12",
            "22d6ea7114b4489299fcaf62139f7433",
            "0753f9ada1e748ef8bb728247efc42f3",
            "f9bdde1880824f9b8e4d240abfb7e3eb",
            "b0923eba24ae4c2490d9aa34eaf8264f",
            "328a1fa427964c4d920856eb7e2fc42e",
            "395d22b9fc0e4fef85699321e4ad1c55",
            "7f137d46be38408d93e19a81756b12f7",
            "63ac20e6a51b4d9d8b32e7296f364fa6",
            "d40f8e1ca19d4f23be82c36d45bcacca",
            "26a8f53b58864f4f9192f6d494e43698",
            "46cc2b6f3c6748b092de0d1d5c865ba1",
            "9ac8ac4d89d7461aa5b980f8b4142d3b",
            "6f1a42edf0bf4dc8b5fd5618ecd4aaf7",
            "52ef381c23be4c7c8ba90a269432144f",
            "3e07f78936594ba3b1ad75b9f359e787",
            "bb4ea284c1a442079279db80e7554751",
            "5cd65beb36aa45ac9eeb1d1fc725b2bb",
            "210c5bcf20dc4ba4b9aa172a185684fd",
            "37e5e313166c465e8bb4fa07232c57a8",
            "fb3f7d0223e44a13911c0dad4c62762c",
            "e75bf78502ae407c9ccbb6108d48368a",
            "7a1c8433709a42ffa2a1859340228ad0",
            "875a86763b454ce6875f114711e90e99",
            "12e060502f38409d9d9f6d00d127024a",
            "0362c0e902a948c893a355b57628ba63",
            "e74a2bb72d6c4cb2846f122cabdac1c9",
            "1fca7209c57743f2a9cd0871b992326f",
            "bf6f2901c35e4ec3b1e6867aa89ca660",
            "cbe28f9f24a24bbfbce5f1debe7bb6a3",
            "2692c5e00cff4a9184eea6e413d447a4",
            "ffba50db994e45f38dc7271cd8b85662",
            "bbf611c8d7784873bff664c1ae2fcc4c",
            "1aa761bb7a554ba4ab8c1421737aba97",
            "3068821bff2447589983d7803655cf0d",
            "07888525b4834dfbb3e15270e6a046b8",
            "b65762c4cc4044519482e316ac513e2f",
            "d40a33cd0ba846ed9bad7aef49652fb1",
            "c5f6e50506774e21b6175e4b42266161",
            "e8880195b1d0430dbd362128e4e6fb71",
            "7900ed0abcd846bfb167157c6188f2b4",
            "7f112632fe6a4226bfc356c979681df5",
            "67557e4c47ce4892800d86689b0f95ae",
            "c8f96e0eb5bc4ac59d1eee0adba55bd2",
            "9f513556046f45cfb7ca202664199010",
            "dcdfee9a03554f07b95090ee26f8eb6b",
            "b6813bf492ea4a10ad67086705fa3913",
            "09043719c8784d1782adc9797003fa73",
            "ea3c39dd094b4bd086bce019e27db007",
            "195c19c3a3554cc0834c070aacecc8f7",
            "6ab7cfd8477f4ababfd890b45dcd04af",
            "7f67027d747b4c878f48317b4266cbf4",
            "1bd6c709b3bd41b98bb62a8f60a997f4",
            "de453b9b33ef44ae90fd7750729bece2",
            "4d792b266d5e4987ad40333f812d6859",
            "01d3f9c8fc08468cab16890e9c20f927",
            "23665863ac0b4975a9a005a6bc488829",
            "ce587cebbb374ce4864b1e040ebdc1f6",
            "550fb552229d4cbbbee8aac918160858",
            "cdfbb676b49f43a0950cd8d7e6f51a73",
            "31e5587dc400416594105d2830e3f543",
            "262e9dcd8b9d4134bc2ae06faa8044f0",
            "3e769fe78c174957a70c42cfee53e706",
            "b437467433b74f36b0d5c48770d30eab",
            "adc1fc707dd14917aa7ae48c3361fe8c",
            "17d27c59ad9b4e35a93505496cb8003e",
            "10451a0f752749caa10202f588fff89a",
            "45daec08105d404cad42680cbf4758b8",
            "801a54a2b1764d73a341919e2fd30a97",
            "6cb78cd1ecae4ef5b30409e63106b9cf",
            "e87a3595db2446db93f6ef343ac7a07e",
            "601b6d23686443acb887ad69d3327d65",
            "bf6198ab2f0c40d6af81c27ec57c2b5d",
            "b9d6142278094f4f862f29f09d8906c9",
            "186fa5bd4e604b5d84b0786de79abc82",
            "0ea78fb9a0ea4dd7b95a3f62a5e3e623",
            "d052bbf4d448408596df4f028fa3d992",
            "8606dc539b4846d3a93e4a441e98d08d",
            "f57d3007680a4819a0a3531f10144f83",
            "2f91b43251f84c5188cdc8db2cb2d27a",
            "ef61930821bf4ff49691b94afda15be6",
            "4acc274f697f4a1c8f55cfa93c2275eb",
            "5f930c7705354d4e9178b43ed646b7d3",
            "fb76cbafc52b45e8a9d3fd357693b370",
            "4ece4e7a33d942a0a7ec2e85294b630e",
            "0c54e7ad26384e77a7288b3269d19648",
            "493eef7bd08b4b009bf297fb5f92b690",
            "9fcb4ef52fb84d3c90514a66e8df1122"
          ]
        },
        "collapsed": true,
        "id": "hqaEZyqF7f0M",
        "outputId": "cfaf27ba-db98-46a4-9edf-7462aa98aaf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3b3ba0bda794d5bbc8b5188c50f6c12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dataset_infos.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46cc2b6f3c6748b092de0d1d5c865ba1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "abstract_algebra/test-00000-of-00001.par(â€¦):   0%|          | 0.00/9.96k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a1c8433709a42ffa2a1859340228ad0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "abstract_algebra/validation-00000-of-000(â€¦):   0%|          | 0.00/3.73k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1aa761bb7a554ba4ab8c1421737aba97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "abstract_algebra/dev-00000-of-00001.parq(â€¦):   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f513556046f45cfb7ca202664199010"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01d3f9c8fc08468cab16890e9c20f927"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/11 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10451a0f752749caa10202f588fff89a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8606dc539b4846d3a93e4a441e98d08d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['test', 'validation', 'dev'])\n",
            "<class 'datasets.arrow_dataset.Dataset'>\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"entfane/professor-mathematics\")\n",
        "\n",
        "# each element in the dataset is of type dict\n",
        "print(dataset[\"train\"][64202])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "bf9be5d5c3db4c07a9b6f732b1a3fe05",
            "cca1a1a4fed94288bf76b289e7a0a6bf",
            "34370a3ccadf44e8ad01b41f9aafcd01",
            "0e6d5000a5564e19af7983f47b0bb2ed",
            "5581b57b9ffb4355ad417ccda13f9bc5",
            "6bb2ecbfbba9410690b9088b72acc361",
            "a352531ae08d49878f264306a6c46feb",
            "8e6ea4e7047f493cb403241625a68964",
            "c920ad22a18547528ec183c5e31b0611",
            "265cde70ed034b949e4eab8c6fd5816f",
            "c5ed59bae14c4e79ad203ec4387b0e0d",
            "9c898bf77f67438794d1b2f5f5913dea",
            "6ceb532a8f234b519f9607539d1b9039",
            "8b6de866b45740a0bf5a7298d63ad40b",
            "3fdbff31056a4994acd8fcdc8ca51c5c",
            "8e512bcda18943269d82590650861d1c",
            "05ecd9dd4bbe4e9ab2eb8d0cc279ce8c",
            "0d29fd6916f74d7ebf428b3b4d1f515c",
            "aafcaa898af943888eed24fd64978370",
            "491ed2f535684ebc9b92546fbdfaf0dc",
            "8c214cc3aada41e9a62c28f27cc03528",
            "e00ca7c3359c46e1b3a11de0a927a0a4",
            "7803aebd7b18452f8c2da4e7126fa2d1",
            "d9621a94ac1649c5ab50e17eecbe9144",
            "9bec506300054538b1d724bbfe846978",
            "6b48c87de1974787bdaadc4da3dcfc65",
            "30811013f8344ace9e49d6a7c6d4adbf",
            "23a5da82a93a4fa284e924f7c7bd55c3",
            "4e0fa551da1a4bd1ab0620dbc47fef9c",
            "537014c085bf4db6a650d65c7432202a",
            "d956d52d2afa413a8a23e4665937544f",
            "00f8ceedc98943f6855a10875d4094a9",
            "41744ec855364674bfd7c5d731ca8658"
          ]
        },
        "id": "2SfokfUa_a-S",
        "outputId": "dfe60b03-b717-4afc-a690-68191b5c8d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/335 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf9be5d5c3db4c07a9b6f732b1a3fe05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/61.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c898bf77f67438794d1b2f5f5913dea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/64203 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7803aebd7b18452f8c2da4e7126fa2d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'At 30, Anika is 4/3 the age of Maddie. What would be their average age in 15 years?', 'answer': \"If Anika is 30 now, in 15 years, she'll be 30+15=<<30+15=45>>45 years old.\\nAt 30, Anika is 4/3 the age of Maddie, meaning Maddie is 4/3*30=<<4/3*30=40>>40 years.\\nIn 15 years, Maddie will be 40+15=<<40+15=55>>55 years old.\\nTheir total age in 15 years will be 55+45=<<55+45=100>>100\\nTheir average age in 15 years will be 100/2=<<100/2=50>>50\\n#### 50\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(((dataset[\"train\"][:2])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D3iFC7gagFE",
        "outputId": "c24f17e8-df5e-4218-8209-3265b9d2e876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': ['Let \\\\( a, b, c \\\\) be positive real numbers. Prove that\\n\\n$$\\n\\\\frac{1}{a(1+b)}+\\\\frac{1}{b(1+c)}+\\\\frac{1}{c(1+a)} \\\\geq \\\\frac{3}{1+abc},\\n$$\\n\\nand that equality occurs if and only if \\\\( a = b = c = 1 \\\\).', 'Given real numbers \\\\( a, b, c \\\\) and a positive number \\\\( \\\\lambda \\\\) such that the polynomial \\\\( f(x) = x^3 + a x^2 + b x + c \\\\) has three real roots \\\\( x_1, x_2, x_3 \\\\), and the conditions \\\\( x_2 - x_1 = \\\\lambda \\\\) and \\\\( x_3 > \\\\frac{1}{2}(x_1 + x_2) \\\\) are satisfied, find the maximum value of \\\\( \\\\frac{2 a^3 + 27 c - 9 a b}{\\\\lambda^3} \\\\).'], 'answer': [\"1. Consider the given inequality:\\n\\n\\\\[\\n\\\\frac{1}{a(1+b)}+ \\\\frac{1}{b(1+c)}+ \\\\frac{1}{c(1+a)} \\\\geq \\\\frac{3}{1 + abc}\\n\\\\]\\n\\nTo simplify, we add \\\\( \\\\frac{3}{1 + abc} \\\\) to both sides. The new inequality becomes:\\n\\n\\\\[\\n\\\\frac{1}{a(1+b)} + \\\\frac{1}{b(1+c)} + \\\\frac{1}{c(1+a)} + \\\\frac{3}{1 + abc} \\\\geq \\\\frac{6}{1 + abc}\\n\\\\]\\n\\n2. Let's analyze each term with an added \\\\( \\\\frac{1}{1 + abc} \\\\):\\n\\n\\\\[\\n\\\\frac{1}{a(1+b)} + \\\\frac{1}{1 + abc}, \\\\quad \\\\frac{1}{b(1+c)} + \\\\frac{1}{1 + abc}, \\\\quad \\\\frac{1}{c(1+a)} + \\\\frac{1}{1 + abc}\\n\\\\]\\n\\nWe can rewrite them as follows:\\n\\n\\\\[\\n\\\\begin{aligned}\\n\\\\frac{1}{a(1+b)} + \\\\frac{1}{1 + abc} &= \\\\frac{1}{1 + abc} \\\\left( \\\\frac{1 + abc}{a(1+b)} + 1 \\\\right), \\\\\\\\\\n\\\\frac{1}{b(1+c)} + \\\\frac{1}{1 + abc} &= \\\\frac{1}{1 + abc} \\\\left( \\\\frac{1 + abc}{b(1+c)} + 1 \\\\right), \\\\\\\\\\n\\\\frac{1}{c(1+a)} + \\\\frac{1}{1 + abc} &= \\\\frac{1}{1 + abc} \\\\left( \\\\frac{1 + abc}{c(1+a)} + 1 \\\\right).\\n\\\\end{aligned}\\n\\\\]\\n\\n3. Simplifying inside the parentheses:\\n\\n\\\\[\\n\\\\begin{aligned}\\n\\\\frac{1 + abc}{a(1+b)} + 1 &= \\\\frac{1 + abc + a(1+b)}{a(1+b)} = \\\\frac{1 + abc + a + ab}{a(1+b)} = \\\\frac{1 + a + ab + abc}{a(1+b)}, \\\\\\\\\\n\\\\frac{1 + abc}{b(1+c)} + 1 &= \\\\frac{1 + abc + b(1+c)}{b(1+c)} = \\\\frac{1 + abc + b + bc}{b(1+c)} = \\\\frac{1 + b + bc + abc}{b(1+c)}, \\\\\\\\\\n\\\\frac{1 + abc}{c(1+a)} + 1 &= \\\\frac{1 + abc + c(1+a)}{c(1+a)} = \\\\frac{1 + abc + c + ac}{c(1+a)} = \\\\frac{1 + c + ac + abc}{c(1+a)}.\\n\\\\end{aligned}\\n\\\\]\\n\\n4. Combining the simplified terms, we get:\\n\\n\\\\[\\n\\\\begin{aligned}\\n\\\\frac{1}{a(1+b)} + \\\\frac{1}{1 + abc} &= \\\\frac{1}{1 + abc} \\\\left( \\\\frac{1 + a + ab + abc}{a(1+b)} \\\\right), \\\\\\\\\\n\\\\frac{1}{b(1+c)} + \\\\frac{1}{1 + abc} &= \\\\frac{1}{1 + abc} \\\\left( \\\\frac{1 + b + bc + abc}{b(1+c)} \\\\right), \\\\\\\\\\n\\\\frac{1}{c(1+a)} + \\\\frac{1}{1 + abc} &= \\\\frac{1}{1 + abc} \\\\left( \\\\frac{1 + c + ac + abc}{c(1+a)} \\\\right).\\n\\\\end{aligned}\\n\\\\]\\n\\n5. Adding these terms together:\\n\\n\\\\[\\n\\\\begin{aligned}\\n\\\\left( \\\\frac{1 + a + ab + abc}{a(1+b)} + \\\\frac{1 + b + bc + abc}{b(1+c)} + \\\\frac{1 + c + ac + abc}{c(1+a)} \\\\right) \\\\cdot \\\\frac{1}{1 + abc}\\n\\\\end{aligned}\\n\\\\]\\n\\nEach term inside the parentheses \\\\( \\\\frac{ 1 + x + xy + xyz}{x(1+y)} \\\\) paired as \\\\( x + \\\\frac{1}{x} \\\\). By AM-GM inequality, for any positive \\\\( x \\\\):\\n\\n\\\\[\\nx + \\\\frac{1}{x} \\\\geq 2\\n\\\\]\\n\\nTherefore, each addition results in at least 2, and since we have three such terms:\\n\\n\\\\[\\n\\\\frac{1 + a + ab + abc}{a(1+b)} + \\\\frac{1 + b + bc + abc}{b(1+c)} + \\\\frac{1 + c + ac + abc}{c(1+a)} \\\\geq 6\\n\\\\]\\n\\nThus, we have:\\n\\n\\\\[\\n\\\\frac{6}{1 + abc}\\n\\\\]\\n\\n6. From steps above, we conclude:\\n\\n\\\\[\\n\\\\frac{1}{a(1+b)} + \\\\frac{1}{b(1+c)} + \\\\frac{1}{c(1+a)} + \\\\frac{3}{1 + abc} \\\\geq \\\\frac{6}{1 + abc}\\n\\\\]\\n\\nThis demonstrates that:\\n\\n\\\\[\\n\\\\boxed{\\\\frac{1}{a(1+b)} + \\\\frac{1}{b(1+c)} + \\\\frac{1}{c(1+a)} \\\\geq \\\\frac{3}{1 + abc}}\\n\\\\]\\n\\nFinally, equality holds if and only if \\\\( a = b = c = 1 \\\\).\", \"\\nWe begin by analyzing the function \\\\( f(x) = x^3 + a x^2 + b x + c \\\\), which has three real roots \\\\( x_1, x_2, x_3 \\\\). We are given the following conditions:\\n1. \\\\( x_2 - x_1 = \\\\lambda \\\\)\\n2. \\\\( x_3 > \\\\frac{1}{2} (x_1 + x_2) \\\\)\\n\\nWe aim to find the maximum value of \\\\( \\\\frac{2a^3 + 27c - 9ab}{\\\\lambda^3} \\\\).\\n\\n1. **Transform the polynomial to remove the quadratic term:**\\n   Substitute \\\\( x = y - \\\\frac{a}{3} \\\\) into \\\\( f(x) \\\\):\\n   \\\\[\\n   \\\\begin{aligned}\\n   F(y) & = f\\\\left(y - \\\\frac{a}{3}\\\\right) \\\\\\\\\\n        & = \\\\left(y - \\\\frac{a}{3}\\\\right)^3 + a \\\\left(y - \\\\frac{a}{3}\\\\right)^2 + b \\\\left(y - \\\\frac{a}{3}\\\\right) + c \\\\\\\\\\n        & = y^3 - \\\\left(\\\\frac{a^2}{3} - b\\\\right)y + \\\\frac{1}{27}(2a^3 + 27c - 9ab).\\n   \\\\end{aligned}\\n   \\\\]\\n\\n2. **Identify the new roots of \\\\( F(y) \\\\):**\\n   Let the roots of \\\\( F(y) \\\\) be \\\\( y_1, y_2, y_3 \\\\). We know \\\\( y_i = x_i + \\\\frac{a}{3} \\\\) for \\\\( i = 1, 2, 3 \\\\). Using Vieta's formulas:\\n   \\\\[\\n   y_1 + y_2 + y_3 = 0 \\n   \\\\]\\n   and \\n   \\\\[\\n   y_1 y_2 y_3 = -\\\\frac{1}{27}(2a^3 + 27c - 9ab).\\n   \\\\]\\n\\n3. **Utilize the conditions provided:**\\n   Using \\\\( x_2 - x_1 = \\\\lambda \\\\):\\n   \\\\[\\n   y_2 - y_1 = \\\\left(x_2 + \\\\frac{a}{3}\\\\right) - \\\\left(x_1 + \\\\frac{a}{3}\\\\right) = x_2 - x_1 = \\\\lambda.\\n   \\\\]\\n   And for \\\\( x_3 \\\\):\\n   \\\\[\\n   y_3 = x_3 + \\\\frac{a}{3} > \\\\frac{1}{2}\\\\left(x_1 + x_2\\\\right) + \\\\frac{a}{3} = \\\\frac{1}{2}\\\\left(y_1 + y_2\\\\right) = -\\\\frac{y_3}{2}.\\n   \\\\]\\n   Thus,\\n   \\\\[\\n   y_3 > 0.\\n   \\\\]\\n\\n4. **Express \\\\( y_1 \\\\) and \\\\( y_2 \\\\) in terms of \\\\( y_3 \\\\) and \\\\( \\\\lambda \\\\):**\\n   From the conditions:\\n   \\\\[\\n   \\\\begin{cases}\\n   y_1 + y_2 + y_3 = 0, \\\\\\\\\\n   y_2 - y_1 = \\\\lambda,\\n   \\\\end{cases}\\n   \\\\]\\n   we solve:\\n   \\\\[\\n   \\\\begin{cases}\\n   y_1 = -\\\\frac{1}{2}(y_3 + \\\\lambda), \\\\\\\\\\n   y_2 = -\\\\frac{1}{2}(y_3 - \\\\lambda).\\n   \\\\end{cases}\\n   \\\\]\\n\\n5. **Calculate \\\\( \\\\frac{2a^3 + 27c - 9ab}{\\\\lambda^3} \\\\):**\\n   \\\\[\\n   \\\\frac{2a^3 + 27c - 9ab}{\\\\lambda^3} = -\\\\frac{27 y_1 y_2 y_3}{\\\\lambda^3}.\\n   \\\\]\\n   Substituting \\\\( y_1 \\\\) and \\\\( y_2 \\\\):\\n   \\\\[\\n   y_1 y_2 = \\\\left(-\\\\frac{1}{2}(y_3 + \\\\lambda)\\\\right) \\\\left(-\\\\frac{1}{2}(y_3 - \\\\lambda)\\\\right) = \\\\frac{1}{4}(y_3^2 - \\\\lambda^2).\\n   \\\\]\\n   Thus,\\n   \\\\[\\n   \\\\frac{2a^3 + 27c - 9ab}{\\\\lambda^3} = -\\\\frac{27}{4} \\\\cdot \\\\frac{y_3^3 - y_3 \\\\lambda^2}{\\\\lambda^3} = -\\\\frac{27}{4} \\\\left(\\\\frac{y_3}{\\\\lambda}^3 - \\\\frac{y_3}{\\\\lambda} \\\\right).\\n   \\\\]\\n\\n6. **Define \\\\( z = \\\\frac{y_3}{\\\\lambda} \\\\):**\\n   Then the expression becomes:\\n   \\\\[\\n   -\\\\frac{27}{4} \\\\left(z^3 - z\\\\right).\\n   \\\\]\\n\\n7. **Maximize \\\\( g(z) = z^3 - z \\\\) for \\\\( z > 0 \\\\):**\\n   \\\\[\\n   g'(z) = 3z^2 - 1 \\\\quad \\\\text{and setting} \\\\quad g'(z) = 0 \\\\quad \\\\text{gives} \\\\quad z = \\\\frac{1}{\\\\sqrt{3}}.\\n   \\\\]\\n   The function \\\\( g(z) \\\\) is strictly decreasing for \\\\( z > \\\\frac{1}{\\\\sqrt{3}} \\\\) and strictly increasing for \\\\( 0 < z < \\\\frac{1}{\\\\sqrt{3}} \\\\). Hence, the minimum value of \\\\( g(z) \\\\) is attained at \\\\( z = \\\\frac{1}{\\\\sqrt{3}} \\\\):\\n   \\\\[\\n   g\\\\left(\\\\frac{1}{\\\\sqrt{3}}\\\\right) = \\\\left(\\\\frac{1}{\\\\sqrt{3}}\\\\right)^3 - \\\\frac{1}{\\\\sqrt{3}} = -\\\\frac{2\\\\sqrt{3}}{9}.\\n   \\\\]\\n\\n8. **Compute the maximum value of the original expression:**\\n   \\\\[\\n   \\\\frac{2a^3 + 27c - 9ab}{\\\\lambda^3} = -\\\\frac{27}{4} \\\\left(-\\\\frac{2\\\\sqrt{3}}{9}\\\\right) = \\\\frac{27 \\\\times 2 \\\\sqrt{3}}{4 \\\\times 9} = \\\\frac{3\\\\sqrt{3}}{2}.\\n   \\\\]\\n\\nConclusion:\\n\\\\[\\n\\\\boxed{\\\\frac{3\\\\sqrt{3}}{2}}\\n\\\\]\"]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"context_length\": 512\n",
        "}\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def format_input(entry):\n",
        "  instruction_text = (\n",
        "      f\"Below is a question that would like a logical answer. Generate a response that provides a step by step solution.\"\n",
        "      f\"\\n\\n### Instruction: {entry['question']}\"\n",
        "  )\n",
        "  return instruction_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def pre_process(entry):\n",
        "  input_text = format_input(entry)\n",
        "  all_text = input_text + entry['answer']\n",
        "  return tokenizer(all_text, return_tensors=\"pt\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-73fEkvlCZWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data, val_data, test_data = split_data(dataset)\n",
        "# train_dataset = dataset[\"train\"][:2]\n",
        "# type(train_data)\n",
        "# print(dataset[\"train\"])\n",
        "print(dataset)\n",
        "split = dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "smalldataset = split[\"train\"].select(range(100))\n",
        "small_train = smalldataset.map(pre_process)\n",
        "smalldataset_val = split[\"test\"].select(range(100))\n",
        "small_val = smalldataset_val.map(pre_process)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "5d9c882124da4ec1bf268e1af8d592f1",
            "c08237923f6746cbb997d4179ea601a3",
            "f8d4b7f8d5394f0b8c91fe2f0af4718c",
            "5efe7da4e06c4c01bb62f5f14b71fb05",
            "8f60c7d67f0e426b96a91ac732cd4d31",
            "8a734171b3964badbb494ad8005b5f4b",
            "62a15eb5e3c5453e9e987c82da0bd693",
            "7058ed9cd1f54e3aa0f849573511cf37",
            "1bfbfe60e1aa4b549794c56156ec10cb",
            "615e544134ae49db90c8262d357a6ec0",
            "27ae8367e8d0449bb6c8fe048ce26051",
            "5356b8250d454bd797d305c30920f9b6",
            "d429a4d42ea947e5bf29d45cd9843877",
            "2d6856ae9ea84e909b13b19eb1de51ac",
            "fbf9a691b513480e970646dfc7cc0192",
            "0370ff4f2135468aaa7b59d88f0af5db",
            "7ef21634b6a74869bcc492368711e964",
            "0a9a6e98d29c4fa8803664391cec7034",
            "911f2f50160d4b81942f450a523a4bf7",
            "6e554261104b464bb2bb526943b16ee6",
            "88813decf0b64d86861dcecd3ed2ff5a",
            "bae6fee9550249909378ec81acdc3e30"
          ]
        },
        "id": "DYPxivoXbIma",
        "outputId": "802a365e-cf67-4d63-e169-7dbadfde69d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['question', 'answer'],\n",
            "        num_rows: 64203\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d9c882124da4ec1bf268e1af8d592f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1152 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5356b8250d454bd797d305c30920f9b6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_train\n",
        "small_train.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
        "small_val.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
        "small_train[0]['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2skCUZ-Cj2v2",
        "outputId": "0ad174a7-e3b2-416b-8389-a9458e3e5c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[21106,   318,   257,  ...,  3467,   737,   198]])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(small_train[0])\n",
        "len(small_train)\n",
        "small_train[0]['input_ids']\n",
        "format_input(smalldataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Nx98MTBmszSw",
        "outputId": "c5b2f7ad-c38e-4df6-e4b3-1d194c5e0d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Below is a question that would like a logical answer. Generate a response that provides a step by step solution.\\n\\n### Instruction: Let \\\\( P(x) \\\\) be a polynomial with integer coefficients. If \\\\( n(P) \\\\) is the number of (distinct) integers \\\\( k \\\\) such that \\\\( P^2(k) = 1 \\\\), prove that\\n\\\\[ n(P) - \\\\operatorname{deg}(P) \\\\leq 2, \\\\]\\nwhere \\\\( \\\\operatorname{deg}(P) \\\\) denotes the degree of the polynomial \\\\( P \\\\).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from functools import partial\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "customized_collate_fn = partial(\n",
        " custom_collate_fn,\n",
        " device=device,\n",
        " allowed_max_length=1024\n",
        ")\n",
        "torch.manual_seed(123)\n",
        "train_loader = DataLoader(small_train, batch_size=1, shuffle=True, collate_fn=customized_collate_fn, drop_last=True)\n",
        "val_loader = DataLoader(small_val, batch_size=6, shuffle=False, collate_fn=customized_collate_fn, drop_last=False)\n",
        "input, target = next(iter(train_loader))\n",
        "print(token_ids_to_text(input, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERanLoaFkpiH",
        "outputId": "fdfadd9c-e26d-47e6-9afa-ef862d229345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is a question that would like a logical answer. Generate a response that provides a step by step solution.\n",
            "\n",
            "### Instruction: Find the smallest natural number \\( n \\) such that whenever the set \\(\\{1, 2, \\cdots, n\\} \\) is divided into any two disjoint subsets, it is always possible to select 3 distinct numbers from one of the subsets where the product of two of them equals the third number.\n",
            "1. Let's consider the problem which seeks to find the smallest natural number \\( n \\) such that when the set \\(\\{1,2, \\cdots, n\\}\\) is arbitrarily divided into two disjoint subsets, it is always possible to find three distinct numbers among them where the product of two of them equals the third one.\n",
            "\n",
            "2. According to the reference solution, if we start by examining \\( n = 95 \\) and attempt to partition the set \\(\\{1,2,\\ldots, 95\\}\\) into two subsets \\( A \\) and \\( B \\):\n",
            "\n",
            "   - For example, one such partition could be:\n",
            "     \\[\n",
            "     \\begin{aligned}\n",
            "     A &= \\{1, 2, 3, 4, 5, 7, 9, 11, 13, 48, 60, 72, 80, 84, 90\\}, \\\\\n",
            "     B &= \\{6, 8, 10, 12, 14, 15, \\ldots, 47, 49, 50, \\ldots, 59, 61, \\ldots, 71, 73, \\ldots, 79, 81, \\\\\n",
            "        & \\quad 82, 83, 85, \\ldots, 89, 91, \\ldots, 95\\}.\n",
            "     \\end{aligned}\n",
            "     \\]\n",
            "   \n",
            "3. From the partition above, it can be observed that no three numbers in \\( A \\) or \\( B \\) individually can satisfy the condition that the product of two of them equals the third.\n",
            "\n",
            "4. Now, if we include \\( 96 \\) in either \\( A \\) or \\( B \\), there will appear numbers which fulfill the equation \\( 2 \\times 48 = 96 \\) or \\( 8 \\times 12 = 96 \\). This shows that the presence of 96 changes the properties of the subsets.\n",
            "\n",
            "5. To show that \\( n = 96 \\) satisfies the condition, we use proof by contradiction as follows:\n",
            "\n",
            "   - Assume \\(\\{1, 2, \\ldots, 96\\} = A \\cup B\\) but there is no such 3-tuple \\((a, b, c)\\) from the same subset \\(A\\) or \\(B\\) satisfying \\( a \\times b = c \\).\n",
            "   - Consider possibilities given the requirement \\( 2 \\times 48 = 96 \\) and similar relationships for combinations of \\(2, 3, 32\\) and \\(2, 3, 48\\):\n",
            "\n",
            "     1. If \\(2, 48 \\in A\\), \\(3\\) and \\(32\\) must be in \\(B\\) (and vice versa). This leads to further constraints which show inequalities in their constraints.\n",
            "     2. Similarly detailing subsets having key numbers like \\(3, 4, 32, 48\\) in \\(A\\) or \\(B\\) exposes contradictions as shown in various cases outlined in the solution.\n",
            "   - For each subset configuration, contradictions like \\( 6, 16, 24, 48, 96 \\in A\\) or \\(B\\) violates the consistency because at least one of such combinations ends forming valid condition sets.\n",
            "\n",
            "6. Therefore, after detailing through such rigorous combinations and constraints, it can be concluded that if \\(n < 96\\), we do not form the necessary partitions or encounter contradictions. For \\(n = 96\\), all subsets tested either in \\(A\\) or \\(B\\) results forming a valid condition satisfying the problem statement.\n",
            "\n",
            "Conclusion:\n",
            "\\[\n",
            "\\boxed{96}\n",
            "\\]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = token_ids_to_text(inputs, tokenizer)\n",
        "print(generated_text[len(input_text):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOZ2GL8aBbnq",
        "outputId": "9583fe15-eaf1-49e9-abe4-2f4e989ae985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The Pythagorean theorem on the right triangles $ABD$ and $CBD$ gives us:\n",
            "\n",
            "\\[(x^\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "torch.manual_seed(123)\n",
        "inputs, targets = next(iter(train_loader))\n",
        "input_ids = text_to_token_ids(format_input(dataset[\"train\"][0]), tokenizer).to(device)\n",
        "output = generate(model=model, idx=text_to_token_ids(format_input(smalldataset[0]), tokenizer).to(device), context_size=1024, max_new_tokens=30, eos_id=50256)\n",
        "print(\"This is output: \", output)\n",
        "generated_text = token_ids_to_text(output, tokenizer)\n",
        "print(\"\\n\\n==============\\n\\n\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5wU28B37KKF",
        "outputId": "59513e91-73d4-41f8-e759-a28b85b33be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is output:  tensor([[21106,   318,   257,  1808,   326,   561,   588,   257, 12219,  3280,\n",
            "            13,  2980,   378,   257,  2882,   326,  3769,   257,  2239,   416,\n",
            "          2239,  4610,    13,   198,   198, 21017, 46486,    25,   554, 39280,\n",
            "         28461,  9248,  9738, 47113,  2173,   720,    35,     3,   290,   720,\n",
            "            36,     3,   389,   319,   262, 18366,   286,   720,  2246,     3,\n",
            "           290,   720,  6242,     3,  8148,    11,   287,   262,   976,  4571,\n",
            "            13,  6910,   720, 14529,     3, 36177,    82,  1627,   720,  5222,\n",
            "             3,   379,   966,   720,    47, 47113,   290,   720, 14529,   796,\n",
            "         18671, 35307,  1649,   720,    47,     3,   318,   319,   262, 14288,\n",
            "           286,  1735,   720,  2749, 47113,   905,   326,   720,  6242,   796,\n",
            "          7125, 35307,   198,   198, 21017, 46486,    25,   554, 39280, 28461,\n",
            "          9248,  9738, 47113,  2173,   720,    35,     3,   290,   720,    36,\n",
            "             3,   389,   319,   262, 18366,   286,   720,  2246,     3,   290,\n",
            "           720,  6242]], device='cuda:0')\n",
            "\n",
            "\n",
            "==============\n",
            "\n",
            "\n",
            "Below is a question that would like a logical answer. Generate a response that provides a step by step solution.\n",
            "\n",
            "### Instruction: In $\\triangle ABC$, points $D$ and $E$ are on the extensions of $AC$ and $AB$ respectively, in the same direction. Line $BD$ intersects line $CE$ at point $P$, and $BD = CE$. When $P$ is on the median of side $BC$, show that $AB = AC$.\n",
            "\n",
            "### Instruction: In $\\triangle ABC$, points $D$ and $E$ are on the extensions of $AC$ and $AB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer = torch.optim.AdamW(\n",
        " model.parameters(), lr=0.00005, weight_decay=0.1\n",
        ")\n",
        "num_epochs = 1\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        " model, train_loader, val_loader, optimizer, device,\n",
        " num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        " start_context=format_input(smalldataset_val[0]), tokenizer=tokenizer\n",
        ")\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "YLM2xTkb4X2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1835fe1-a7a4-47ff-e735-88bee020da75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.268, Val loss 2.001\n",
            "Ep 1 (Step 000005): Train loss 2.366, Val loss 1.955\n",
            "Ep 1 (Step 000010): Train loss 2.401, Val loss 1.911\n",
            "Ep 1 (Step 000015): Train loss 1.809, Val loss 1.866\n",
            "Ep 1 (Step 000020): Train loss 1.966, Val loss 1.827\n",
            "Ep 1 (Step 000025): Train loss 1.695, Val loss 1.801\n",
            "Ep 1 (Step 000030): Train loss 1.872, Val loss 1.779\n",
            "Ep 1 (Step 000035): Train loss 2.223, Val loss 1.762\n",
            "Ep 1 (Step 000040): Train loss 1.793, Val loss 1.752\n",
            "Ep 1 (Step 000045): Train loss 2.204, Val loss 1.743\n",
            "Ep 1 (Step 000050): Train loss 2.184, Val loss 1.734\n",
            "Ep 1 (Step 000055): Train loss 2.068, Val loss 1.727\n",
            "Ep 1 (Step 000060): Train loss 1.695, Val loss 1.720\n",
            "Ep 1 (Step 000065): Train loss 1.844, Val loss 1.713\n",
            "Ep 1 (Step 000070): Train loss 1.856, Val loss 1.706\n",
            "Ep 1 (Step 000075): Train loss 1.741, Val loss 1.698\n",
            "Ep 1 (Step 000080): Train loss 1.860, Val loss 1.692\n",
            "Ep 1 (Step 000085): Train loss 1.724, Val loss 1.686\n",
            "Ep 1 (Step 000090): Train loss 1.865, Val loss 1.681\n",
            "Ep 1 (Step 000095): Train loss 1.755, Val loss 1.675\n",
            "Training completed in 1.27 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = text_to_token_ids(format_input(smalldataset[0]), tokenizer)\n",
        "output_ids = generate(model=model, idx=input_text.to(device), context_size=1024, max_new_tokens=100, eos_id=50256)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "x90qv8bUZOCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(token_ids_to_text(output_ids, tokenizer)[len(format_input(smalldataset[0])):])\n",
        "print(\"\\n\\n============\\n\\n\")\n",
        "print(smalldataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CiLVX9EZFmj",
        "outputId": "25f32d12-1c49-4a0e-b531-20b8b12cb980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "### Instruction: Let \\( P(x) \\) be a polynomial with integer coefficients.\n",
            "\n",
            "### Instruction: Let \\( P(x) \\) be a polynomial with integer coefficients.\n",
            "\n",
            "### Instruction: Let \\( P(x) \\) be a polynomial with integer coefficients.\n",
            "### Instruction: Let \\( P(x) \\) be a polynomial with integer coefficients.\n",
            "\n",
            "### Instruction: Let \\( P(x) \\ be\n",
            "\n",
            "\n",
            "============\n",
            "\n",
            "\n",
            "{'question': 'Let \\\\( P(x) \\\\) be a polynomial with integer coefficients. If \\\\( n(P) \\\\) is the number of (distinct) integers \\\\( k \\\\) such that \\\\( P^2(k) = 1 \\\\), prove that\\n\\\\[ n(P) - \\\\operatorname{deg}(P) \\\\leq 2, \\\\]\\nwhere \\\\( \\\\operatorname{deg}(P) \\\\) denotes the degree of the polynomial \\\\( P \\\\).', 'answer': '\\n1. **Understanding the problem**:\\n   - We are given a polynomial \\\\( P(x) \\\\) with integer coefficients.\\n   - Define \\\\( n(P) \\\\) as the number of distinct integers \\\\( k \\\\) such that \\\\( P^2(k) = 1 \\\\).\\n   - We need to prove that \\\\( n(P) - \\\\operatorname{deg}(P) \\\\leq 2 \\\\), where \\\\( \\\\operatorname{deg}(P) \\\\) is the degree of polynomial \\\\( P \\\\).\\n\\n2. **Case when \\\\(\\\\operatorname{deg}(P) \\\\leq 2\\\\)**:\\n   - For any polynomial \\\\( P \\\\) of degree 2, the degree of \\\\( P^2 \\\\) is \\\\( 2 \\\\operatorname{deg}(P) \\\\).\\n   - Therefore, for \\\\(\\\\operatorname{deg}(P) = d \\\\),\\n     \\\\[\\n     n(P) \\\\leq 2d\\n     \\\\]\\n   - Since \\\\( 2d \\\\leq d + 2 \\\\) when \\\\( d \\\\leq 2 \\\\), the statement holds.\\n\\n3. **Case when \\\\(\\\\operatorname{deg}(P) \\\\geq 3\\\\)**:\\n   - Assume \\\\( \\\\operatorname{deg}(P) \\\\geq 3 \\\\) and \\\\( n(P) > \\\\operatorname{deg}(P) + 2 \\\\).\\n   - This assumption implies that \\\\( P(x) \\\\) has at least 3 integer roots where \\\\( P(x) = 1 \\\\) and at least 3 integer roots where \\\\( P(x) = -1 \\\\).\\n\\n4. **Polynomial decomposition**:\\n   - Without loss of generality, we assume \\\\( P(0) = -1 \\\\) (if not, consider \\\\( P(x+b) \\\\)).\\n   - Let \\\\( k_1, k_2, \\\\ldots, k_m \\\\) be the integers such that \\\\( P(k_i) = 1 \\\\).\\n   - Then, we write:\\n     \\\\[\\n     P(x) = Q(x)(x - k_1)(x - k_2) \\\\cdots (x - k_m) + 1\\n     \\\\]\\n   - Set \\\\( x = 0 \\\\):\\n     \\\\[\\n     P(0) = Q(0)(-k_1)(-k_2) \\\\cdots (-k_m) + 1\\n     \\\\]\\n     \\\\[\\n     -1 = Q(0)(-1)^m k_1 k_2 \\\\cdots k_m + 1\\n     \\\\]\\n   - Simplify to get:\\n     \\\\[\\n     -2 = (-1)^m Q(0) k_1 k_2 \\\\cdots k_m\\n     \\\\]\\n   - Thus, \\\\( k_1 k_2 \\\\cdots k_m \\\\mid 2 \\\\). This implies \\\\( m \\\\leq 3 \\\\).\\n\\n5. **Symmetry in \\\\( P(x) \\\\) and \\\\(-P(x) \\\\)**:\\n   - Analogous to \\\\( P(x)=1 \\\\), consider \\\\( P(x) = -1 \\\\).\\n   - Apply the same process and conclude that it also contributes at most 3 solutions.\\n   - Therefore, in total, \\\\( P^2(x) = 1 \\\\) has at most 6 integer solutions for \\\\(\\\\operatorname{deg}(P) \\\\geq 3\\\\).\\n\\n6. **Case for \\\\(\\\\operatorname{deg}(P) = 3\\\\)**:\\n   - If \\\\( \\\\operatorname{deg}(P) = 3\\\\), then in our assumption \\\\( n(P) = 6 \\\\).\\n   - From the algebraic analysis, forms like \\\\( P(x) = -((x^2 - 1)(x - 2)) + 1 \\\\) and other similar forms produce at most 4 distinct integer roots.\\n   - Verifying these forms show it contradicts \\\\( n(P) = 6 \\\\).\\n\\n7. **Conclusion**:\\n   - Hence, the claim holds by validating that \\\\( \\\\boxed{n(P) - \\\\operatorname{deg}(P) \\\\leq 2} \\\\).\\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch, pad_token_id=50256,\n",
        "                      ignore_index=-100,\n",
        "                      allowed_max_length=None,\n",
        "                      device=\"cpu\"):\n",
        "  batch_max_length = max(item['input_ids'].shape[1] + 1 for item in batch)\n",
        "  if allowed_max_length is not None:\n",
        "    batch_max_length = min(batch_max_length, allowed_max_length+1)\n",
        "\n",
        "\n",
        "  inputs_lst, targets_lst = [], []\n",
        "  for item in batch:\n",
        "    new_item = item['input_ids'][0].clone()\n",
        "    pad_length = batch_max_length - new_item.shape[0]\n",
        "    padding = torch.full((max(0, pad_length),), pad_token_id)\n",
        "    padded = torch.cat([new_item[:batch_max_length], padding], dim=0)\n",
        "    inputs = padded[:-1].clone()\n",
        "    targets = padded[1:].clone()\n",
        "    targets[targets.shape[0]-1] = pad_token_id\n",
        "    mask = targets == pad_token_id\n",
        "    indices = torch.nonzero(mask).squeeze()\n",
        "    if indices.numel() > 1:\n",
        "      targets[indices[1:]] = ignore_index\n",
        "\n",
        "\n",
        "    inputs_lst.append(inputs)\n",
        "    targets_lst.append(targets)\n",
        "  inputs_tensor = torch.stack(inputs_lst, dim=0).to(device)\n",
        "  targets_tensor = torch.stack(targets_lst, dim=0).to(device)\n",
        "  return inputs_tensor, targets_tensor\n"
      ],
      "metadata": {
        "id": "TgyScLhje1Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy_batch(input_batch, target_batch, model, device):\n",
        "  input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "  logits = model(input_batch).logits\n",
        "  preds = torch.argmax(logits, dim=-1)"
      ],
      "metadata": {
        "id": "73CnFZPqePUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch).logits\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest logits value\n",
        "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            output = model(idx_cond)\n",
        "            if isinstance(output, transformers.utils.ModelOutput):\n",
        "                logits = output.logits\n",
        "            else:\n",
        "                logits = output\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()  # Calculate loss gradients\n",
        "            optimizer.step()  # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        # generate_and_print_sample(\n",
        "        #     model, tokenizer, device, start_context\n",
        "        # )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text)\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n"
      ],
      "metadata": {
        "id": "KbkR2MjZrmQk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}