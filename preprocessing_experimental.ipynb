{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5yI15v+mLGm8nCw1M8qEW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlinapp/proofLLM/blob/main/preprocessing_experimental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "55103009e08a4ea8af635cc8287870d6",
            "7b0a4addec3748ec85fb0b294b1b7936",
            "775703b79cee4dac964f5ca06d9ab31d",
            "948c75af222a4d96b795593e1b0ccf49",
            "84e81693cdd24a30ac395c4b996efba9",
            "3e3d8e973d7f4fe5ab933f10a25d0917",
            "ab3b7c2e3ac54822b0c6ed996566c507",
            "bd8b87e851054abd9a93f1abab453c46",
            "f4c675688c554db8ace74b49bfa46d7e",
            "76949a9e0f544dcf8aed8720a24254c2",
            "7a118a4fb7504e1bac09637e1ca29f97",
            "4d4490e1d35a4e1e96dfd5a5ecd68ced",
            "528fdd7063a142689ff6126036ebefda",
            "31102af34dc74ccf9b056aa0bd97ee93",
            "ca7a589c324a4cbc882b75ee7619afeb",
            "8c4cc39d60c34b11820642291dfa066d",
            "7b94a1fe82ae49fa84e2625f7fdb1c82"
          ]
        },
        "id": "1nlbUAzoJzSg",
        "outputId": "b6f4d758-5e77-4794-9540-77ba563fe9f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55103009e08a4ea8af635cc8287870d6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tiktoken\n",
        "import importlib\n",
        "import torch\n",
        "import transformers\n",
        "from huggingface_hub import notebook_login\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "print(\"tiktoken version:\", importlib.metadata.version('tiktoken'))\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = load_dataset(\"entfane/professor-mathematics\", split=\"train\")\n",
        "# train_data = dataset.select(range(20))\n",
        "\n",
        "# train_data = InstructionDataset(train_data, tokenizer)\n",
        "\n",
        "# torch.manual_seed(123)\n",
        "# train_loader = DataLoader(train_data, batch_size=2, shuffle=True, collate_fn=custom_collate_fn)\n",
        "# inputs, targets = next(iter(train_loader))\n",
        "# inputs[0][-7:], targets[0][-7:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFx2ITVpM2Ij",
        "outputId": "7ee5f278-0ab8-46f7-dd4a-2e19c9d03d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n",
              " tensor([-100, -100, -100, -100, -100, -100, -100]))"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(type(dataset))\n",
        "# print(isinstance(dataset, datasets.dataset_dict.DatasetDict))\n",
        "# print(dataset[0]['question'], \"\\n\\n\")\n",
        "# encoded_ids = tokenizer.encode(dataset[0]['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKX3MHCsNa_H",
        "outputId": "5996ea62-a270-46c9-ca6f-0bbf1dd81821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "737"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "VZWDgcuhIaUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = (\"<|endoftext|>\")\n",
        "output = tokenizer.encode(input, allowed_special={\"<|endoftext|>\"})\n",
        "strings = tokenizer.decode(output)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pov9mTB-Ir61",
        "outputId": "4028d637-3677-46db-a63b-dae0d46f1fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mK9WXzuZVTUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Dataset stored internally as python list\n",
        "'''\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "  def __init__(self, dataset, tokenizer, max_length=1024):\n",
        "    self.input_ids = []\n",
        "    if (isinstance(dataset, datasets.arrow_dataset.Dataset)):\n",
        "      for entry in dataset:\n",
        "        formatted_entry = format_input(entry)\n",
        "        input_id = tokenizer.encode(formatted_entry, allowed_special={\"<|endoftext|>\"})\n",
        "        if (len(input_id) > max_length):\n",
        "          continue\n",
        "        self.input_ids.append(input_id)\n",
        "    else:\n",
        "      print(\"Not datasets.arrow_dataset.Dataset class. Did not add\")\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx]"
      ],
      "metadata": {
        "id": "V6F-igpSLu80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(input) -> str:\n",
        "  return (\"###Question:\\n\" + input['question'] + \"\\n\\n###Answer:\\n\" + input['answer'])\n",
        "\n",
        "\n",
        "\n",
        "def custom_collate_fn(batch, pad_token_id=50256,\n",
        "                      ignore_index=-100,\n",
        "                      allowed_max_length=None,\n",
        "                      device=\"cpu\"):\n",
        "  batch_max_length = max(len(item) + 1 for item in batch)\n",
        "  if allowed_max_length is not None:\n",
        "    batch_max_length = min(batch_max_length, allowed_max_length+1)\n",
        "\n",
        "\n",
        "  inputs_lst, targets_lst = [], []\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "    new_item += [pad_token_id]\n",
        "    padded = new_item + ([pad_token_id] * (batch_max_length - len(new_item)))\n",
        "    inputs = torch.tensor(padded[:-1]);\n",
        "    targets = torch.tensor(padded[1:])\n",
        "\n",
        "\n",
        "    mask = targets == pad_token_id\n",
        "    indices = torch.nonzero(mask).squeeze()\n",
        "    if indices.numel() > 1:\n",
        "      targets[indices[1:]] = ignore_index\n",
        "    if allowed_max_length is not None:\n",
        "      inputs = inputs[:allowed_max_length]\n",
        "      targets = targets[:allowed_max_length]\n",
        "\n",
        "\n",
        "    inputs_lst.append(inputs)\n",
        "    targets_lst.append(targets)\n",
        "  inputs_tensor = torch.stack(inputs_lst, dim=0).to(device)\n",
        "  targets_tensor = torch.stack(targets_lst, dim=0).to(device)\n",
        "  return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "D6ynVRyaLOeE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}